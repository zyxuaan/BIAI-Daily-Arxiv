---
layout: default
title: 2025-08-12 Arxiv论文摘要
---

# Arxiv论文总结报告

## 基本信息
- 生成时间: 2025-08-12 12:32:51
- 使用模型: gemini-2.5-flash
- 论文数量: 1 篇

---

## 论文总结

### [[LET-US: Long Event-Text Understanding of Scenes]](http://arxiv.org/abs/2508.07401v1)
<!-- 2025-08-10 -->
**📅 发布日期**: 2025-08-10

*   **👥 作者**: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu
*   **🎯 研究目的**: 事件相机输出具有微秒级时间分辨率的稀疏、异步事件流，这使得低延迟和高动态范围的视觉感知成为可能。然而，尽管现有的多模态大型语言模型（MLLMs）在理解和分析RGB视频内容方面取得了显著成功，但它们要么无法有效解释事件流，要么仅限于处理非常短的序列。本研究旨在弥合事件数据与文本之间存在的巨大模态鸿沟，提出一个能够对长事件流进行文本理解的框架，从而实现对扩展事件序列的跨模态推理理解。
*   **⭐ 主要发现**: 本文提出了LET-US框架，这是一个用于长事件流-文本理解的创新性框架。LET-US的核心创新在于其采用了自适应压缩机制，该机制能够在显著减少输入事件数据量的同时，有效保留关键的视觉细节。这一突破性方法使得LET-US能够克服现有MLLM在处理长事件序列时的局限性，从而在扩展事件序列的跨模态推理理解方面开辟了新的前沿。

---

---

## 生成说明
- 本报告由AI模型自动生成，摘要内容仅供参考。
- 如有错误或遗漏，请以原始论文为准。
