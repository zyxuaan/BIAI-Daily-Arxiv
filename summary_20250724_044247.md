---
layout: default
title: 2025-07-24 Arxiv论文摘要
---

# Arxiv论文总结报告

## 基本信息
- 生成时间: 2025-07-24 12:42:55
- 使用模型: gemini-2.5-flash
- 论文数量: 1 篇

---

## 论文总结

### [[Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras]](http://arxiv.org/abs/2507.17664v1)
<!-- 2025-07-23 -->
**📅 发布日期**: 2025-07-23

*   **👥 作者**: Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau
*   **🎯 研究目的**: 事件相机以其微秒级延迟和对运动模糊的鲁棒性，成为理解动态环境的理想选择。然而，将这些异步事件流与人类语言关联起来仍然是一个开放的挑战。本研究旨在解决这一问题，通过引入首个大规模基准和相应的框架，实现基于事件感知的语言驱动目标定位，从而弥合事件数据与人类语言理解之间的鸿沟。
*   **⭐ 主要发现**: 本文推出了 **Talk2Event**，这是首个用于事件相机感知中语言驱动目标定位的大规模基准数据集。该基准基于真实世界的驾驶数据构建，提供了超过30,000个经过验证的指代表达，每个表达都富含四种定位属性——外观、状态、与观察者的关系以及与其他物体的关系，从而促进了空间、时间和关系推理。为了充分利用这些线索，研究团队还提出了 **EventRefer**，一个属性感知的定位框架，它通过“事件-属性专家混合”（Mixture of Event-Attribute Expert）机制动态融合多属性表示。这些创新为事件相机数据与人类语言理解的结合奠定了基础，有望显著提升动态场景下的感知能力。

---

---

## 生成说明
- 本报告由AI模型自动生成，摘要内容仅供参考。
- 如有错误或遗漏，请以原始论文为准。
