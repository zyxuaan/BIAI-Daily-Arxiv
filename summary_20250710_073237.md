---
layout: default
title: 2025-07-10 Arxiv论文摘要
---

# Arxiv论文总结报告

## 基本信息
- 生成时间: 2025-07-10 15:51:00
- 使用模型: gemini-2.5-flash
- 论文数量: 100 篇

---

## 论文总结

### [[EA: An Event Autoencoder for High-Speed Vision Sensing]](http://arxiv.org/abs/2507.06459v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-07-09

*   **👥 作者**: Riadul Islam, Joey Mulé, Dhandeep Challagundla, Shahmir Rizvi, Sean Carson
*   **🎯 研究目的**: 高速视觉感知对于机器人、自动驾驶和工业自动化等领域的实时感知至关重要。然而，传统的基于帧的视觉系统面临运动模糊、高延迟和数据冗余等挑战，限制了它们在动态环境中的性能。事件相机作为一种新兴技术，通过异步捕获像素级亮度变化提供了有前景的替代方案，但其输出的稀疏且嘈杂的事件流给目标检测带来了困难。本研究旨在解决这些问题，提出一种能够高效压缩和重建事件数据，同时有效保留关键时空特征的架构，以提升事件相机在高速视觉感知应用中的性能。
*   **⭐ 主要发现**: 本论文提出了一种名为“事件自编码器”（EA: An Event Autoencoder）的新型架构，旨在克服事件相机数据在目标检测中的稀疏性和噪声挑战。该模型的核心贡献在于其能够高效地压缩和重建事件数据，同时精确地保留了重要的空间和时间特征。具体而言，EA模型采用了卷积编码（convolutional encoding）机制，并创新性地结合了自适应阈值选择（adaptive threshold selection）以及一个轻量级分类器（lightweight classifier）。这些设计使得EA能够有效地处理事件流的特性，从而为高动态环境下的实时感知提供了更鲁棒和高效的解决方案，有望显著提升事件相机在目标检测等应用中的性能。

---
### [[Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach]](http://arxiv.org/abs/2507.03765v1)
**📅 发布日期**: 2025-07-04

*   **👥 作者**: Hebei Li, Yansong Peng, Jiahui Yuan, Peixi Wu, Jin Wang, Yueyi Zhang, Xiaoyan Sun
*   **🎯 研究目的**: 事件相机因其高时间分辨率等优势，已被引入图像语义分割领域。然而，现有基于事件的语义分割方法未能充分利用帧和事件提供的互补信息，导致训练策略复杂且计算成本高昂。为解决这些挑战，本研究旨在提出一种高效的混合神经网络框架，以有效融合帧和事件数据，从而实现更高效的图像语义分割。
*   **⭐ 主要发现**: 本文提出了一种新颖且高效的混合神经网络框架，用于图像语义分割。该框架独特地结合了脉冲神经网络（SNN）分支来处理事件数据，以及人工神经网络（ANN）分支来处理帧数据。为促进这两个分支间的深度交互和信息融合，研究引入了三个专门设计的模块：自适应时间加权（ATW）注入器、事件驱动稀疏（EDS）注入器，以及（摘要中未完全展示的）通道交互模块。这一混合方法旨在通过充分利用帧和事件的互补优势，克服现有方法的局限性，实现更高效、更低计算成本的语义分割。

---
### [[Interpolation-Based Event Visual Data Filtering Algorithms]](http://arxiv.org/abs/2507.01557v1)
**📅 发布日期**: 2025-07-02

*   **👥 作者**: Marcin Kowlaczyk, Tomasz Kryjak
*   **🎯 研究目的**: 神经形态视觉领域发展迅速，事件相机正被广泛应用于各种场景。然而，这些传感器的数据流存在显著的噪声问题。本文旨在提出一种针对事件数据流的有效去噪方法，以解决事件相机数据中噪声过高的问题，同时尽可能保留有效信号，从而提高事件数据的可用性和质量。
*   **⭐ 主要发现**: 本文提出了四种基于无限脉冲响应（IIR）滤波器矩阵的事件数据过滤算法。这些算法在实验中表现出色，能够去除约99%的噪声，同时保留了大部分有效信号。研究人员在多个事件数据集上对这些算法进行了比较，包括添加了人工生成噪声和通过动态视觉传感器（DVS）记录的真实噪声的数据集，验证了其有效性。值得注意的是，所提出的方法对内存占用极低，对于1280x720分辨率的传感器，仅需约30KB内存。因此，这些算法非常适合在嵌入式设备中实现和部署，为事件相机的实际应用提供了高效且低成本的去噪解决方案。

---
### [[EvRWKV: 事件引导低光图像增强的RWKV框架]](http://arxiv.org/abs/2507.03184v1)
**📅 发布日期**: 2025-07-01

*   **👥 作者**: WenJie Cai, Qingguo Meng, Zhenyu Wang, Xingbo Dong, Zhe Jin
*   **🎯 研究目的**: 在低光照环境下捕获高质量视觉内容仍然是一个严峻的挑战，因为图像会受到严重噪声、运动模糊和曝光不足的影响，从而降低下游应用的性能。传统的基于帧的低光增强方法往往会放大噪声或无法保留结构细节，尤其是在真实世界场景中。事件相机通过异步捕获亮度变化，提供高动态范围和微秒级时间分辨率，被认为是低光成像的有前景替代方案。然而，现有的事件-图像融合方法存在融合策略过于简单以及对时空错位和噪声处理不足的问题。本研究旨在解决这些挑战，提出EvRWKV框架，以实现有效的事件引导低光图像增强。
*   **⭐ 主要发现**: 本研究提出了EvRWKV，一个新颖的框架，旨在有效利用事件数据来增强低光图像。该框架通过实现连续的跨模态交互，克服了现有事件-图像融合方法中融合策略简单以及对时空错位和噪声处理不足的局限性。EvRWKV的核心创新在于其独特的设计，能够更精细地融合事件信息和图像信息，从而在低光照条件下生成高质量、低噪声且细节保留完好的图像。尽管摘要未完全提供所有实验结果，但可以推断，EvRWKV有望显著提升低光图像的视觉质量，并为依赖高品质视觉输入的下游应用提供更可靠的基础，从而推动低光成像领域的发展。

---
### [[Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking]](http://arxiv.org/abs/2506.23783v1)
**📅 发布日期**: 2025-06-30

*   **👥 作者**: Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang
*   **🎯 研究目的**: 近年来，结合传统RGB相机和仿生事件相机进行鲁棒目标跟踪受到了越来越多的关注。然而，现有的大多数多模态跟踪算法严重依赖于高复杂度的Vision Transformer架构进行特征提取和跨模态融合，这不仅导致巨大的计算开销，也限制了跨模态交互的有效性。本研究旨在解决这些问题，提出一个基于线性复杂度Vision Mamba网络的高效RGB-事件目标跟踪框架，命名为Mamba-FETrack V2，以期在保证跟踪性能的同时，显著降低计算成本并提升跨模态交互效率。
*   **⭐ 主要发现**: 本文的核心贡献在于提出了Mamba-FETrack V2，一个基于线性复杂度Vision Mamba网络的高效RGB-事件目标跟踪框架。与现有依赖高复杂度Vision Transformer的方法不同，Mamba-FETrack V2显著降低了计算开销并优化了跨模态交互。其创新点之一是设计了一个轻量级的Prompt Generator，该生成器利用每种模态的嵌入特征以及一个共享的提示池，动态生成模态特定的可学习提示，从而有效促进了不同模态间的信息融合与交互。

---
### [[Event-based Tiny Object Detection: A Benchmark Dataset and Baseline]](http://arxiv.org/abs/2506.23575v1)
**📅 发布日期**: 2025-06-30

*   **👥 作者**: Nuo Chen, Chao Xiao, Yimian Dai, Shiman He, Miao Li, Wei An
*   **🎯 研究目的**: 反无人机任务中的小目标检测（SOD）因无人机尺寸小巧和背景复杂而极具挑战性。传统基于帧的相机由于帧率低、动态范围有限和数据冗余，难以在复杂环境中有效检测小目标。尽管事件相机凭借其微秒级时间分辨率和高动态范围，为SOD提供了更有效的解决方案，但现有事件目标检测数据集普遍存在规模有限、目标尺寸偏大且背景多样性不足的问题，使其不适用于小目标检测的基准测试。本研究旨在解决这一空白，为反无人机任务提供一个大规模、高多样性的事件相机小目标检测基准数据集。
*   **⭐ 主要发现**: 本文的主要贡献是引入了一个名为EV-UAV的事件小目标检测（EVSOD）数据集。这是首个专门针对反无人机任务设计的大规模、高多样性基准数据集，旨在弥补现有事件目标检测数据集的不足。EV-UAV数据集包含147个序列，以及超过230万（此处原文截断，但已表明数据量巨大）的数据。该数据集的发布为事件相机在小目标检测领域的研究提供了关键的、高质量的资源，有望显著推动反无人机任务中事件相机应用的进一步发展和相关算法的性能提升。

---
### [[STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene]](http://arxiv.org/abs/2506.23157v1)
**📅 发布日期**: 2025-06-29

*   **👥 作者**: Hanyu Zhou, Haonan Wang, Haoyue Liu, Yuxing Duan, Luxin Yan, Gim Hee Lee
*   **🎯 研究目的**: 高动态场景重建旨在精确表示静态背景的刚性空间特征和动态物体的变形连续时空特征。然而，现有方法通常采用统一的表示模型（如高斯），直接从帧相机数据中匹配动态场景的时空特征。这种统一范式在处理帧成像导致的物体潜在时间特征不连续性以及背景与物体之间异构的空间特征时面临挑战，导致时空特征不匹配。本研究旨在解决这些问题，通过解耦时空特征并引入事件相机来弥补帧相机的不足，从而实现更精确、鲁棒的高动态场景重建。
*   **⭐ 主要发现**: 为了解决现有高动态场景重建方法中存在的时空特征不匹配问题，本文提出了STD-GS（时空解耦高斯泼溅）方法。其核心创新点和发现包括：
    *   **时空特征解耦**: 针对静态背景与动态物体之间异构的空间特征以及帧成像导致的物体潜在时间特征不连续性，STD-GS将场景的时空特征解耦为多种潜在表示，有效缓解了背景与动态物体之间存在的时空特征不匹配问题。
    *   **帧-事件相机融合**: 创新性地引入事件相机来补偿传统帧相机的不足。事件相机能够提供高时间分辨率的异步数据流，这对于捕捉物体潜在的不连续时间特征至关重要，从而弥补了帧相机在处理快速运动或光照变化场景时的局限性。
    *   **高斯泼溅新范式**: 通过结合上述时空特征解耦和帧-事件相机融合策略，STD-GS为高动态场景重建提供了一种新的高斯泼溅范式，能够更准确、精细地表示静态背景和动态物体的复杂时空特性，从而提升了高动态场景的重建质量。

---
### [[Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission]](http://arxiv.org/abs/2506.20222v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-25

*   **👥 作者**: Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu
*   **🎯 研究目的**: 随着事件相机与RGB相机在各类视觉应用中的结合日益广泛，混合系统面临的主要挑战在于如何高效传输海量的触发事件和RGB图像数据。传统上，这两种相机以不同方式捕获同一场景，导致其输出数据中存在显著的冗余信息。本研究旨在提出一种创新的传输方案，以解决混合事件-RGB数据传输中的带宽效率和冗余问题，同时确保两种数据源的有效重建性能，并实现并行实时去模糊。
*   **⭐ 主要发现**: 为了应对混合事件-RGB数据传输中的挑战，本研究提出了一种联合事件与图像（E-I）传输框架。该框架的核心创新在于能够有效消除RGB图像和事件数据之间存在的冗余信息，从而显著优化数据传输过程。通过这种方式，该方案不仅能够保持两种数据源的高效重建性能，还能够支持并行实时去模糊，为混合视觉系统的数据传输效率和应用性能带来了潜在的提升。

---
### [EvDetMAV: Generalized MAV Detection from Moving Event Cameras](http://arxiv.org/abs/2506.19416v2)
**📅 发布日期**: 2025-06-24

*   **👥 作者**: Yin Zhang, Zian Ning, Xiaoyu Zhang, Shiliang Guo, Peidong Liu, Shiyu Zhao
*   **🎯 研究目的**: 现有的微型飞行器（MAV）检测方法主要依赖于RGB图像中目标的外观特征，但由于外观特征的多样性，难以实现通用化的MAV检测。本研究注意到，不同类型的MAV由于其高速旋转的螺旋桨，在事件流中表现出共同且独特的特征，而这些特征在RGB图像中难以捕捉。因此，本文旨在研究如何充分利用原始事件流中螺旋桨的显著时空特征，从事件相机数据中检测不同类型的MAV，从而解决现有方法在泛化性上的局限。
*   **⭐ 主要发现**: 本文提出了一种新颖的基于事件相机的MAV检测方法 EvDetMAV。该方法创新性地利用了MAV高速旋转螺旋桨在事件流中产生的独特且通用的时空特征，克服了传统RGB方法在泛化性上的局限。EvDetMAV包含三个核心模块，旨在有效提取螺旋桨的显著时空特征，同时过滤掉背景物体和相机运动产生的噪声。鉴于现有事件相机MAV数据集的缺失，本研究还引入了一个新颖的MAV数据集，为未来的事件相机MAV检测研究提供了宝贵的资源。这一研究为实现更鲁棒、更通用的MAV检测提供了新的视角和解决方案，特别是在低光照、高动态范围或高速运动场景下，事件相机的优势将得到充分发挥。

---
### [[Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation]](http://arxiv.org/abs/2506.18443v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-23

*   **👥 作者**: Yang Lyu, Zhenghao Zou, Yanfeng Li, Chunhui Zhao, Quan Pan
*   **🎯 研究目的**: 针对敏捷机器人（例如特技飞行器）的可靠自我运动估计面临巨大挑战，因为大多数机器人传感器在高动态运动下难以及时清晰地响应，常导致测量模糊、失真和延迟。本研究旨在提出一种无需IMU（惯性测量单元）且无需特征关联的框架，通过结合事件相机和毫米波雷达这两种外部传感器，实现机器人在高动态场景下的激进自我运动速度估计。
*   **⭐ 主要发现**: 本文提出了一种创新的方法，利用事件相机的瞬时原始事件数据和毫米波雷达的多普勒测量数据，直接推导机器人的旋转和平移速度。该方法无需在测量帧之间进行复杂的特征关联过程，显著增强了系统在无纹理和无结构环境中的鲁棒性。这一IMU-free和feature-association-free的框架为敏捷机器人在极端动态条件下实现精确的自我运动估计提供了新的解决方案，有望提升此类机器人的自主导航和控制性能。

---
### [[When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network]](http://arxiv.org/abs/2506.17457v1)
**📅 发布日期**: 2025-06-20

*   **👥 作者**: Dong Xiao, Guangyao Chen, Peixi Peng, Yangru Huang, Yifan Zhao, Yongxing Dai, Yonghong Tian
*   **🎯 研究目的**: 自动驾驶系统的安全性和可靠性离不开高效的异常检测。然而，当前多数异常检测方法过度关注检测精度，却忽视了在时间敏感型驾驶场景中至关重要的响应速度。本论文的核心研究目的在于提出一种兼顾极低响应时间与高检测准确率的实时异常检测方案，以提升自动驾驶系统的即时决策能力和安全性。
*   **⭐ 主要发现**: 为解决实时性挑战，论文提出了一种创新的多模态异步混合网络（Multimodal Asynchronous Hybrid Network）。该网络巧妙地融合了事件相机（Event Camera）产生的高时间分辨率事件流数据与RGB相机捕捉的图像数据。具体而言，该网络利用异步图神经网络（Asynchronous Graph Neural Network）处理事件相机数据，以捕捉其固有的高时间分辨率动态特性；同时，通过卷积神经网络（CNN）从RGB图像中提取丰富的空间特征。这种独特的融合机制使得模型能够全面捕捉驾驶环境中的时间动态和空间细节，从而在保证高准确率的同时，显著提升异常检测的实时响应能力，为自动驾驶系统的安全运行提供了关键技术支持。

---
### [[An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras]](http://arxiv.org/abs/2506.16436v1)
<!-- 论文发布日期，格式：2025-06-19 -->
**📅 发布日期**: 2025-06-19

*   **👥 作者**: Antonio Giulio Coretti, Mattia Varile, Mario Edoardo Bertaina
*   **🎯 研究目的**: 空间碎片对航天活动构成严重威胁，促使人们积极研究主动和被动缓解策略。本研究旨在开发一种创新的碰撞规避系统，该系统利用新兴的事件相机技术，特别适用于空间态势感知（SSA）和空间交通管理（STM），以有效探测和规避微弱的移动空间物体。
*   **⭐ 主要发现**: 本文的核心贡献在于提出并实现了一种结合Stack-CNN算法与事件相机的碰撞规避系统。该系统能够有效分析实时事件相机数据，以探测微弱的移动物体。值得注意的是，Stack-CNN算法此前曾成功应用于流星探测，此次将其引入碰撞规避领域是其创新应用。通过在地面数据上的测试，研究证明了该算法能够显著提高信噪比，这对于识别微弱空间目标至关重要。这一方法为未来的星载成像提供了有前景的解决方案，并有望显著提升空间态势感知（SSA）和空间交通管理（STM）操作的效率和准确性。

---
### [[How Real is CARLA's Dynamic Vision Sensor? A Study on the Sim-to-Real Gap in Traffic Object Detection]](http://arxiv.org/abs/2506.13722v1)
**📅 发布日期**: 2025-06-16

*   **👥 作者**: Kaiyuan Tan, Pavan Kumar B N, Bharatesh Chakravarthi
*   **🎯 研究目的**: 本研究旨在系统性地评估CARLA驾驶模拟器内置动态视觉传感器（DVS）模块在交通目标检测应用中的“仿真到现实”（sim-to-real）鸿沟。事件相机因其低延迟、高时间分辨率和高能效，在交通监控，特别是交通路口实时目标检测中展现出巨大潜力。然而，开发鲁棒的基于事件的检测模型面临挑战，主要是因为缺乏带标注的真实世界数据集。为解决此问题，已开发出多种仿真工具来生成合成事件数据，其中CARLA DVS是重要一员。尽管CARLA DVS有潜力，但其在事件目标检测领域的“仿真到现实”鸿沟尚未得到充分研究。本研究的核心目标正是填补这一空白，通过深入评估来理解合成事件数据与真实世界数据之间的差异，从而为开发更可靠的基于事件的交通目标检测模型奠定基础。
*   **⭐ 主要发现**: 根据提供的摘要，本文的核心贡献在于对CARLA DVS在事件目标检测中的“仿真到现实”鸿沟进行了系统性评估。虽然摘要未详细阐述具体的实验结果和发现，但该研究通过训练一个循环视觉转换器（摘要在此处中断），旨在量化和理解使用仿真数据训练的模型在真实世界场景中的性能表现。其潜在影响在于为理解和弥合合成事件数据与真实世界数据之间的差距提供关键见解，从而促进更可靠、更实用的基于事件的交通目标检测模型的发展，为未来在真实世界部署事件相机系统提供指导。
### [Sparse Convolutional Recurrent Learning for Efficient Event-based Neuromorphic Object Detection](http://arxiv.org/abs/2506.13440v1)
<!-- 2025-06-16 -->
**📅 发布日期**: 2025-06-16

*   **👥 作者**: Shenqi Wang, Yingfu Xu, Amirreza Yousefzadeh, Sherif Eissa, Henk Corporaal, Federico Corradi, Guangzhi Tang
*   **🎯 研究目的**: 事件相机凭借其高时间分辨率和动态范围，能够显著提升汽车和机器人应用在实际场景中的目标检测性能和安全性。然而，处理稀疏事件数据需要计算密集型的卷积循环单元，这使得它们难以集成到资源受限的边缘应用中。为解决这一挑战，本文提出了一种名为稀疏事件高效检测器（Sparse Event-based Efficient Detector, SEED）的方法，旨在为神经形态处理器上的高效事件目标检测提供解决方案。
*   **⭐ 主要发现**: 本文的核心贡献是引入了“稀疏卷积循环学习”（sparse convolutional recurrent learning）方法。该方法在循环处理中实现了超过92%的激活稀疏性，从而大幅降低了稀疏事件数据进行时空推理所需的计算成本。研究人员在Prophesee的1 Mpx和Gen1事件目标检测数据集上验证了所提出方法的有效性。

---
### [基于事件相机和强化学习的人机导航](http://arxiv.org/abs/2506.10790v1)
**📅 发布日期**: 2025-06-12

*   **👥 作者**: Ignacio Bugueno-Cordova, Javier Ruiz-del-Solar, Rodrigo Verschae
*   **🎯 研究目的**: 本研究旨在开发一种新型机器人导航控制器，以解决传统基于图像的控制器在固定帧率下易受运动模糊和高延迟影响的问题。该控制器结合了事件相机、其他距离传感器与强化学习技术，核心目标是实现实时、以人为中心（human-centered）的导航和避障功能。其关键创新在于利用事件相机的异步特性，能够以灵活的时间间隔处理视觉信息，从而实现自适应的推理和控制，克服传统方法的局限性。
*   **⭐ 主要发现**: 本研究提出的导航框架巧妙地整合了基于事件的感知、额外的距离传感以及通过深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）进行的策略优化。为提高学习效率，该方法还引入了一个初始的模仿学习（Imitation Learning）阶段来提升样本效率。在模拟环境中进行的实验取得了令人鼓舞的结果，成功展示了系统在复杂场景下实现鲁棒导航和行人跟随的能力，验证了该结合事件相机与强化学习方法的有效性和潜力。

---
### [[WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras]](http://arxiv.org/abs/2506.09098v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-10

*   **👥 作者**: Yangjie Cui, Boyang Gao, Yiwei Zhang, Xin Dong, Jinwu Xiang, Daochun Li, Zhan Tu
*   **🎯 研究目的**: 事件相机感知在利用密集事件表示进行目标检测方面已展现出一定性能。然而，现有研究对这些密集表示中累积的噪声问题关注不足，这会导致表示质量下降并增加漏检的可能性。为了解决这一关键挑战，本研究旨在提出一种名为WD-DETR（Wavelet Denoising-enhanced DEtection TRansformer）的新型网络，专门用于事件相机，通过有效的小波去噪技术，提升机器人感知中目标检测的实时性和准确性。
*   **⭐ 主要发现**: 本文的核心贡献是提出了WD-DETR网络，一个创新性地将小波去噪技术融入到实时目标检测Transformer框架中的模型。具体而言，主要发现和创新点包括：
    *   首先，提出了一种高效的密集事件表示方法，能够实现事件数据的实时张量重建，为后续处理奠定基础。
    *   其次，设计并引入了一种新颖的小波变换方法，专门用于滤除事件表示中积累的噪声，显著提升了数据质量。
    *   更重要的是，该小波去噪方法被巧妙地集成到骨干网络中，用于特征提取阶段，确保在特征生成之初就有效抑制噪声。
    *   提取出的去噪特征随后被送入基于Transformer的检测模块进行目标检测。
    *   通过有效解决事件相机密集表示中的噪声问题，WD-DETR有望显著提高事件相机在复杂环境下进行目标检测的准确性和鲁棒性，从而为机器人感知提供更可靠的视觉信息。
### [[Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera]](http://arxiv.org/abs/2506.08327v1)
**📅 发布日期**: 2025-06-10

*   **👥 作者**: Yuto Kase, Kai Ishibe, Ryoma Yasuda, Yudai Washida, Sakiko Hashimoto
*   **🎯 研究目的**: 在网球等球拍运动中，精确地定位网球击打球拍时的位置对于分析球员和装备特性至关重要，有助于个性化装备设计。传统上，高速摄像机被用于测量击球点，但这种方法存在诸多限制：内存消耗过大导致无法长时间捕获场景，手动数字化耗时且易出错。这些缺点使得难以有效捕捉整个比赛场景，从而阻碍了对球员表现的深入分析。本研究旨在解决这些问题，提出一种利用事件相机实时定位网球击打球拍位置的新方法，以克服现有技术的局限性，从而更好地分析球员表现和装备特点。
*   **⭐ 主要发现**: 本研究的核心贡献在于提出了一种创新的方法，利用事件相机（Event Camera）实时定位网球击打球拍时的准确位置。事件相机与传统帧式相机不同，它能以微秒级的精度高效测量亮度变化（即“事件”），特别适用于捕捉高速运动，同时具有低内存消耗的优点。这项技术克服了传统高速摄像机在内存消耗、手动数据处理耗时和误差方面的局限性。通过实现实时、高效且准确的击球点定位，该方法有望彻底改变网球运动分析的方式，使得能够长时间捕捉比赛场景，消除人工数字化过程，减少人为错误，从而更有效地分析球员表现，并为个性化装备设计提供更精确的数据支持。

---
### [[Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow]](http://arxiv.org/abs/2506.07878v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-09

*   **👥 作者**: Muhammad Ahmed Humais, Xiaoqian Huang, Hussain Sajwani, Sajid Javed, Yahya Zweiri
*   **🎯 研究目的**: 针对事件相机在实时应用中进行低延迟运动估计（光流）的需求，本研究旨在解决当前方法的局限性。具体而言，虽然传统的深度学习范式（如CNN、RNN、ViT）在性能上表现出色，但其计算效率往往无法满足实时应用的要求；而异步事件基方法（包括SNNs和GNNs）尽管计算高效，却难以捕获足够丰富的时空信息，这对于实现高精度的光流估计至关重要。因此，本研究的核心目标是提出一种新的模型，能够有效结合计算效率与强大的时空信息捕获能力，从而为事件基光流估计提供更优的解决方案。
*   **⭐ 主要发现**: 论文的核心贡献是引入了**时空状态空间模型（Spatio-Temporal State Space Model, STSSM）模块**。该模块旨在有效捕获事件数据中的时空信息，从而在保持计算效率的同时，显著提升事件基光流估计的性能。STSSM有望弥补现有深度学习方法在效率上的不足以及现有事件基方法在时空信息捕获上的缺陷，为实时、高性能的光流估计提供新的解决方案，为事件相机在各种实时应用中的广泛部署奠定基础。

---
### [[基于自我中心事件视觉的乒乓球轨迹预测]](http://arxiv.org/abs/2506.07860v1)
**📅 发布日期**: 2025-06-09

*   **👥 作者**: Ivan Alberico, Marco Cannici, Giovanni Cioffi, Davide Scaramuzza
*   **🎯 研究目的**: 传统的相机在处理高速运动物体（如乒乓球）时，常面临高延迟和运动模糊的问题，导致难以实现准确的实时轨迹预测。本研究旨在开发一个利用事件相机优势的实时自我中心乒乓球轨迹预测系统，以克服传统相机的局限性。通过利用事件相机的高时间分辨率、更频繁的状态更新以及对异常值的鲁棒性，该系统旨在仅凭对手击球后的短时间窗口数据，即可实现乒乓球轨迹的精确预测。最终，本研究旨在提升高速运动场景下的视觉感知能力，为体育分析、机器人控制及增强现实等领域提供更准确、更实时的解决方案。
*   **⭐ 主要发现**: 本文提出了一种创新的实时自我中心乒乓球轨迹预测系统，该系统核心在于利用事件相机，而非传统相机。研究发现，事件相机在高球速下表现出卓越的性能，能够提供更高的时间分辨率，从而实现更频繁的状态更新、更强的异常值鲁棒性，并能在对手击球后的极短时间内准确预测轨迹。为支持这项研究，作者收集了一个独特的乒乓球比赛序列数据集，其中包含了球的3D真实轨迹，并与Meta Project Aria眼镜的传感器数据和事件流进行了同步，为未来研究提供了宝贵的资源。此外，该系统还创新性地引入了“中心凹视觉”（foveated vision）概念，利用眼镜的眼动注视数据，仅处理观看者中心凹区域的事件。这种受生物学启发的处理方法显著提升了乒乓球的检测性能，并有望进一步优化轨迹预测的准确性。这项工作不仅展示了事件相机在高速运动分析中的巨大潜力，也为实时体育分析、高性能机器人视觉系统以及沉浸式AR/VR体验开辟了新的技术路径。

---
### [[Reading in the Dark with Foveated Event Vision]](http://arxiv.org/abs/2506.06918v1)
**📅 发布日期**: 2025-06-07

*   **👥 作者**: Carl Brander, Giovanni Cioffi, Nico Messikommer, Davide Scaramuzza
*   **🎯 研究目的**: 当前智能眼镜配备的RGB摄像头在低光照和高速运动场景下，由于运动模糊和动态范围有限，难以有效感知环境。同时，帧式摄像头捕获图像需要高带宽和大量功耗，导致电池续航能力差。这些问题尤其阻碍了从图像中读取文本（OCR）算法的开发和应用。本研究旨在解决这些挑战，提出一种新颖的、基于事件的智能眼镜光学字符识别（OCR）方法。核心目标是利用事件相机在高动态和快速场景下的优势，实现高效、低功耗的文本识别，从而提升智能眼镜在复杂环境下的“阅读”能力。
*   **⭐ 主要发现**: 本研究的核心贡献在于提出了一种新颖的、基于事件的智能眼镜光学字符识别（OCR）方法。该方法通过利用用户的眼球凝视（eye gaze）来“中心凹化”（foveate）事件流，实现了显著的带宽优化，将数据传输量减少了约98%。这一创新性方法充分利用了事件相机在高动态范围和高速运动场景下固有的优势，有效克服了传统帧式相机在这些条件下的局限性（如运动模糊和动态范围不足）。（虽然摘要被截断，但论文明确指出其提出的方法执行“深度二值重建”，暗示了在事件数据处理和文本重建方面的技术突破。）这项工作为智能眼镜在低光照和快速移动等恶劣环境下实现鲁棒、高效的文本识别提供了可行方案，有望大幅提升智能眼镜的实用性和用户体验，并显著延长设备续航时间。

---
### [[EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras]](http://arxiv.org/abs/2506.06596v1)
**📅 发布日期**: 2025-06-07

*   **👥 作者**: Youssef Farah, Federico Paredes-Vallés, Guido De Croon, Muhammad Ahmed Humais, Hussain Sajwani, Yahya Zweiri
*   **🎯 研究目的**: 事件相机作为一种新型仿生传感器，以其高时间分辨率和对亮度变化的异步响应，在运动相关任务（如运动分割）中展现出巨大潜力。然而，训练基于事件的神经网络面临着获取高质量真值数据成本高昂、易出错且频率受限的挑战。本文旨在解决这一真值数据稀缺的问题，提出一种自监督的卷积神经网络（CNN），用于实现基于事件相机的运动分割。
*   **⭐ 主要发现**: 本文的核心贡献是引入了EV-LayerSegNet，一个专为事件相机运动分割设计的自监督CNN模型。该模型创新性地借鉴了场景动态的分层表示思想，能够独立学习仿射光流和分割掩码。其关键机制在于，利用学习到的光流和分割掩码对输入事件进行去模糊处理，并将去模糊的质量作为自监督信号，从而有效地避免了对外部真值数据的依赖。这一自监督学习范式有望显著降低训练成本，推动事件相机在复杂运动场景分割任务中的应用。

---
### [[Bidirectional Image-Event Guided Low-Light Image Enhancement]](http://arxiv.org/abs/2506.06120v1)
**📅 发布日期**: 2025-06-06

*   **👥 作者**: Zhanwen Liu, Huanna Song, Yang Wang, Nan Yang, Shangyu Xie, Yisheng An, Xiangmo Zhao
*   **🎯 研究目的**: 在极低光照条件下，传统基于帧的相机由于动态范围和时间分辨率的限制，捕获的图像容易出现细节丢失和运动模糊。为克服这一瓶颈，研究人员引入了事件相机并提出了事件引导的低光图像增强算法。然而，这些现有方法忽略了动态光照条件引起的全局低频噪声以及稀疏事件数据中局部结构不连续性的影响。本文旨在解决这些问题，提出一种创新的双向引导低光图像增强框架（BiLIE），以提升极低光照下图像的质量和细节。
*   **⭐ 主要发现**: 本文提出了一种创新的双向引导低光图像增强框架（BiLIE），旨在有效解决现有事件引导方法中存在的全局低频噪声和稀疏事件数据导致的局部结构不连续性问题。具体而言，为了减轻全局光照阶跃变化引入的显著低频噪声，该框架引入了基于频率高通滤波的事件特征增强模块。尽管摘要截断，但从“双向图像-事件引导”的命名推断，BiLIE框架可能还结合了图像信息来弥补事件数据的稀疏性，从而更好地恢复图像细节和结构，实现更鲁棒、高质量的低光图像增强。这项工作为极低光照下的图像处理提供了新的思路和解决方案。

---
### [[FRED: The Florence RGB-Event Drone Dataset]](http://arxiv.org/abs/2506.05163v1)
**📅 发布日期**: 2025-06-05

*   **👥 作者**: Gabriele Magrini, Niccolò Marini, Federico Becattini, Lorenzo Berlincioni, Niccolò Biondi, Pietro Pala, Alberto Del Bimbo
*   **🎯 研究目的**: 针对小型、快速、轻量级无人机在传统RGB相机下难以捕捉的问题（尤其是在快速移动和复杂光照条件下），以及现有基准数据集在时间分辨率和无人机特定运动模式方面的不足，本文旨在引入一个专为无人机检测、跟踪和轨迹预测设计的新型多模态数据集，以推动相关领域的研究进展。
*   **⭐ 主要发现**: 本文核心贡献是推出了Florence RGB-Event Drone (FRED) 数据集。FRED是一个新颖的多模态数据集，专门为无人机检测、跟踪和轨迹预测任务设计，结合了RGB视频和事件流数据。该数据集包含超过7小时的密集标注无人机轨迹，涵盖了5种不同型号的无人机，并包含了雨天等具有挑战性的场景。FRED数据集的发布有望弥补现有基准数据集的不足，为研究人员在复杂环境下开发和评估基于事件相机和RGB融合的无人机感知算法提供宝贵的资源，从而推动相关领域的技术进步。

---
### [Spike-TBR: a Noise Resilient Neuromorphic Event Representation](http://arxiv.org/abs/2506.04817v2)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-05

*   **👥 作者**: Gabriele Magrini, Federico Becattini, Luca Cultrera, Lorenzo Berlincioni, Pietro Pala, Alberto Del Bimbo
*   **🎯 研究目的**: 事件相机相比传统帧式传感器具有高时间分辨率、低延迟和高动态范围等显著优势。然而，将事件流高效转换为与标准计算机视觉管线兼容的格式仍然是一个挑战，尤其是在存在噪声的情况下。本文旨在解决这一问题，提出一种名为Spike-TBR的新型事件编码策略，其核心目标是通过整合脉冲神经元来克服现有时间二值表示（TBR）对噪声的脆弱性，从而为事件流创建一种更鲁棒、更抗噪声的表示方法。
*   **⭐ 主要发现**: 本文的核心贡献在于提出了Spike-TBR，这是一种新颖的、基于事件的编码策略，它在现有时间二值表示（TBR）的基础上进行了创新性改进。Spike-TBR巧妙地将TBR的帧式表示优势与脉冲神经网络（SNN）固有的噪声过滤能力相结合，从而显著增强了事件流表示的鲁棒性。研究人员通过集成不同类型的脉冲神经元，开发并评估了Spike-TBR的四种变体，并在多个数据集上进行了验证（尽管摘要未详述具体结果，但强调了其在噪声环境下的优越性）。这一方法为将事件相机数据高效且抗噪声地应用于传统计算机视觉任务提供了新的途径，有望推动事件相机在复杂环境中的应用。

---
### [[EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects]](http://arxiv.org/abs/2506.04048v1)
**📅 发布日期**: 2025-06-04

*   **👥 作者**: Gabriele Magrini, Federico Becattini, Giovanni Colombo, Pietro Pala
*   **🎯 研究目的**: 监测空中物体对安全、野生动物保护和环境研究至关重要。然而，传统的基于RGB的视觉方法在处理尺度变化、运动模糊和高速物体移动（特别是昆虫和无人机等小型飞行实体）时面临诸多挑战。本研究旨在探索基于事件的视觉技术在检测和识别飞行物体（尤其是那些行为模式难以预测的动物）方面的潜力，以克服传统方法的局限性。事件相机因其高时间分辨率、低延迟和对运动模糊的鲁棒性，被认为非常适合这项任务。
*   **⭐ 主要发现**: 为了推动基于事件的飞行物体识别研究，本研究引入了一个名为EV-Flying的全新事件数据集。该数据集包含了手动标注的鸟类、昆虫和无人机，并提供了详细的时空边界框和轨迹身份信息。EV-Flying数据集的创建旨在为研究人员提供一个宝贵的资源，以开发和评估针对野外飞行物体识别的基于事件的视觉算法，从而解决传统RGB相机在处理高速、小尺寸飞行物时遇到的关键挑战，并促进该领域的技术进步。

---
### [[Probabilistic Online Event Downsampling]](http://arxiv.org/abs/2506.02547v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-06-03

*   **👥 作者**: Andreu Girbau-Xalabarder, Jun Nagata, Shinichi Sumiyoshi
*   **🎯 研究目的**: 事件相机以其异步、高时间分辨率的特性在捕捉场景变化方面具有独特优势，但这也导致了巨大的带宽、内存和计算开销。为缓解这一问题，事件下采样技术应运而生。然而，现有方法多依赖固定的启发式规则或阈值策略，导致其适应性受限。本文旨在提出一种更灵活、自适应的概率框架，以高效地对事件流进行下采样，从而有效管理事件相机生成的海量数据。
*   **⭐ 主要发现**: 本文提出了一种名为POLED（Probabilistic Online Event Downsampling）的概率框架，用于解决事件相机数据过载问题。其核心创新在于：
    1.  **概率建模**: POLED通过引入“事件重要性概率密度函数（ePDF）”来建模事件的重要性，该函数可以根据不同应用需求任意定义和调整，极大地增强了方法的灵活性和适应性。
    2.  **纯在线操作**: 该框架能够在纯在线环境下，从原始事件流中实时（on-the-fly）估计事件的重要性，从而实现针对特定场景的动态自适应下采样。
    3.  **零样本能力**: 此外，该方法还引入了零样本事件下采样的能力（摘要中提及，但具体细节未展开）。
    这些创新使得POLED能够提供一种高效、自适应且灵活的事件下采样解决方案，有望显著降低事件相机的数据处理负担，并在资源受限的应用中发挥关键作用。

---
### [[S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification]](http://arxiv.org/abs/2505.24401v1)
**📅 发布日期**: 2025-05-30

*   **👥 作者**: Xianheng Ma, Hongchen Tan, Xiuping Liu, Yi Zhang, Huasheng Wang, Jiang Liu, Ying Chen, Hantao Liu
*   **🎯 研究目的**: 该研究旨在利用事件相机在抵抗恶劣光照条件、减少背景干扰、实现高时间分辨率以及保护面部信息方面的独特优势，深入探究基于事件的长序列行人重识别（Re-ID）任务。其核心目标是开发一个简单且高效的模型，以有效处理此类长序列事件数据，从而克服传统相机在复杂环境下的局限性，提升行人识别在挑战性环境中的性能和隐私保护能力。
*   **⭐ 主要发现**:
    *   **核心贡献**: 论文提出了一种名为“脉冲引导时空语义耦合与扩展网络”（S3CE-Net）的简单高效长序列事件行人重识别模型。
    *   **技术创新**: S3CE-Net基于脉冲神经网络（SNNs）构建，旨在更好地处理事件相机产生的异步事件数据流，这与传统神经网络处理帧式数据的方式有所不同，更符合事件数据的特性。
    *   **关键组件**: 模型内部集成了两个核心组件：
        *   “脉冲引导时空注意力机制”（SSAM）：该机制旨在实现空间和时间维度上的语义交互与关联，从而更有效地捕捉和整合行人特征。
        *   “时空特征采样策略”（STFS）：该策略用于优化特征的提取和表示，进一步提升模型的识别能力。
    *   **潜在影响**: S3CE-Net的提出为基于事件的长序列行人重识别任务提供了一个新颖且高效的解决方案，有望在恶劣光照、高动态场景以及对隐私保护有较高要求的应用中，显著提升行人识别的准确性和鲁棒性。

---
### [Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling](http://arxiv.org/abs/2505.21187v1)
**📅 发布日期**: 2025-05-27

* **👥 作者**: Hesam Araghi, Jan van Gemert, Nergis Tomen
* **🎯 研究目的**: 事件相机以其高时间分辨率和高能效，非常适用于边缘AI应用。然而，其高事件率给数据传输和处理带来了挑战。虽然下采样方法提供了一种实用的解决方案，但其对下游视觉任务的影响尚未得到充分探索。本研究旨在系统地评估六种硬件友好的下采样方法，并利用卷积神经网络在多个基准数据集上进行事件视频分类，以探究下采样对准确性的影响。研究还假设高密度区域的事件携带更多与任务相关的信息，因此更适合进行下采样。
* **⭐ 主要发现**: 本文系统地评估了六种硬件友好的下采样方法，并利用卷积神经网络在各种基准数据集上进行了事件视频分类任务。研究结果表明，高密度区域的事件确实携带了更多与任务相关的信息。基于这一假设，作者提出了一种简单且因果的（causal）基于密度的下采样方法。实验证明，在稀疏事件流的情况下，该方法能够显著提高分类准确性，为事件相机数据处理中的数据效率与准确性平衡提供了新的视角和实用解决方案。

---
### [[EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction]](http://arxiv.org/abs/2505.19169v3)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-05-25

*   **👥 作者**: Ryosei Hara, Wataru Ikeda, Masashi Hatano, Mariko Isogawa
*   **🎯 研究目的**: 三维手部网格重建是人机交互和AR/VR应用中的一项重要但具挑战性的任务。传统的RGB和/或深度摄像头在此任务中面临低光照环境和运动模糊的挑战。近年来，事件相机因其高动态范围和高时间分辨率而备受关注，有望解决这些局限性。然而，现有基于事件相机的方法通常受限于背景噪声或相机运动，导致研究多限于静态背景和固定相机条件。本研究旨在克服这些限制，提出一种新颖的、基于事件相机的、在自我中心视角下进行三维手部网格重建的方法，以期在更复杂的动态环境中实现鲁棒的手部重建。
*   **⭐ 主要发现**: 本研究提出了EventEgoHands，一种创新的、基于事件相机的自我中心视角三维手部网格重建方法。该方法旨在解决现有事件相机在处理背景噪声和相机运动时的敏感性问题，从而突破了以往研究仅限于静态背景和固定相机的局限。通过引入如手部分割（Hand Segmentation）等关键技术，EventEgoHands能够有效地利用事件相机的高动态范围和高时间分辨率优势，在复杂的自我中心视角下实现准确的手部三维重建。这项工作为AR/VR和人机交互领域在高动态、低光照等挑战性环境下进行手部追踪和交互提供了新的解决方案，具有重要的理论和应用价值。
### [[Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras]](http://arxiv.org/abs/2505.17582v1)
**📅 发布日期**: 2025-05-23

*   **👥 作者**: Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato
*   **🎯 研究目的**: 随着自动驾驶技术的日益普及，先进的传感器技术对于确保其安全和可靠运行至关重要。当前，融合激光雷达、雷达和摄像头等多种传感器的技术虽然已被证明有效，但其硬件复杂性和成本较高。因此，开发一种能够承担多种角色的单一传感器，对于实现成本效益高且可扩展的自动驾驶系统具有重要意义。本研究旨在探索利用事件相机，结合相位相关法，实现户外驾驶环境下的距离估计，以期解决传统多传感器融合系统在硬件复杂性和成本方面的挑战，并充分利用事件相机在高动态范围、低延迟和高时间分辨率等方面的独特优势，尤其是在低光照或背光等挑战性照明条件下的卓越性能。
*   **⭐ 主要发现**: 论文指出，事件相机凭借其独特的高动态范围、低延迟和高时间分辨率特性，在传统传感器难以应对的恶劣照明条件下（如低光照或背光环境）展现出卓越的性能。基于此，研究提出利用事件相机，并结合相位相关（Phase-only Correlation）方法，进行户外驾驶环境中的距离估计。这一方法旨在开发一种多功能单一传感器，以替代复杂的传感器融合系统，从而有效降低自动驾驶系统的硬件成本和复杂性，提升其可扩展性。

---
### [Event-Driven Dynamic Scene Depth Completion](http://arxiv.org/abs/2505.13279v2)
**📅 发布日期**: 2025-05-19

*   **👥 作者**: Zhiqiang Yan, Jianhao Jiao, Zhengxue Wang, Gim Hee Lee
*   **🎯 研究目的**: 在动态场景中进行深度补全面临巨大挑战，因为快速的自我运动和物体运动会严重降低RGB图像和LiDAR测量等输入模态的质量。传统的RGB-D传感器在这种条件下难以精确对齐并捕获可靠的深度信息。为了解决这一问题，研究人员注意到事件相机具有高时间分辨率和像素级运动敏感性，能够在动态环境中提供有益的互补线索。基于此，本文旨在提出一个利用事件相机数据进行深度补全的新框架，以克服传统方法的局限性。
*   **⭐ 主要发现**: 本文的核心贡献是提出了EventDC，这是首个专门为动态场景深度补全设计的事件驱动框架。该框架由两个关键模块组成：事件调制对齐（Event-Modulated Alignment, EMA）和局部深度滤波（Local Depth Filtering, LDF）。值得注意的是，这两个模块都能够自适应地学习卷积操作中的基本组成部分，即偏移量（offsets）和权重（weights），这使得它们在处理复杂动态数据时更具鲁棒性和适应性。EventDC的提出，利用了事件相机在高速运动下捕捉精细运动细节的独特优势，为解决传统RGB-D传感器在动态环境中深度信息获取的难题开辟了新途径，有望显著提升动态场景下深度补全的质量和可靠性。

---
### [[Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach]](http://arxiv.org/abs/2505.12903v1)
**📅 发布日期**: 2025-05-19

*   **👥 作者**: Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo
*   **🎯 研究目的**: 现有视觉目标跟踪算法普遍依赖低帧率RGB相机和计算密集型深度神经网络，这导致了固有的高延迟问题，并且在资源受限的环境中表现不佳。为了克服这些局限性，近年来，利用仿生事件相机进行视觉目标跟踪成为一个有前景的研究方向，其在低延迟应用中具有显著优势。本文旨在提出一种新颖的“慢-快跟踪”范式（SFTrack），旨在实现低延迟的事件流视觉目标跟踪，并能灵活适应不同的操作需求。
*   **⭐ 主要发现**: 本文提出了一种名为SFTrack的新型“慢-快跟踪”范式，旨在解决传统基于帧的视觉跟踪方法在高延迟和资源受限环境下的挑战。SFTrack框架支持两种互补的跟踪模式：一种是适用于计算资源充足场景的“高精度慢速跟踪器”，另一种（根据上下文推断）应是为低延迟或资源受限场景设计的快速跟踪器。这种双模式设计使得SFTrack能够灵活地在跟踪精度和实时性之间进行权衡，从而更好地适应多样化的应用场景。该方法为利用事件相机实现低延迟、高适应性的视觉目标跟踪提供了新的解决方案。
### [[Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset]](http://arxiv.org/abs/2505.12588v1)
**📅 发布日期**: 2025-05-19

*   **👥 作者**: Samya Bagchi, Peter Anastasiou, Matthew Tetlow, Tat-Jun Chin, Yasir Latif
*   **🎯 研究目的**: 航天器抖动会严重影响其精确定位能力，而这种能力对于光学通信、地球观测和空间态势感知等任务至关重要。开发有效的抖动估计和补偿算法，需要高保真、能真实反映机载抖动的传感器观测数据。然而，目前缺乏此类高质量数据集。本研究旨在解决这一数据空白，首次提出了一个基于事件相机的星空观测数据集——e-STURT（Event-based Star Tracking Under Jitter），该数据集在受控抖动条件下收集，旨在为抖动估计和补偿算法的开发提供必要的、代表性的数据。
*   **⭐ 主要发现**: 本论文的核心贡献是发布了e-STURT数据集，这是第一个在受控抖动条件下，利用事件相机进行星空观测的数据集。为创建该数据集，研究团队采用了专门的硬件，能够精确模拟事件相机在航天器机载抖动下的运行状态。事件相机提供了异步、高时间分辨率的星空观测数据，同时通过微米级精度的压电执行器引入了系统性、可重复的抖动。尽管摘要未完全展开，但已明确指出该数据集通过模拟不同抖动源，为开发和评估先进的抖动估计与补偿算法提供了独特的、宝贵的资源，有望显著推动航天器精确定位技术的发展。

---
### [MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection](http://arxiv.org/abs/2505.11282v2)
**📅 发布日期**: 2025-05-16

*   **👥 作者**: Shrutarv Awasthi, Anas Gouda, Sven Franke, Jérôme Rutinowski, Frank Hoffmann, Moritz Roidl
*   **🎯 研究目的**: 随着Unitree B2和Fraunhofer O3dyn等移动机器人平台的速度达到每秒5到10米，如何有效利用这些高速性能成为一个挑战。传统的RGB相机在高动态环境下存在运动模糊和实时响应不足的问题，限制了其在高速度感知中的应用。事件相机凭借其异步操作和低延迟感知特性，为高速机器人感知提供了有前景的替代方案。本研究旨在解决高速、高动态环境下机器人精确感知（尤其是6D姿态估计和运动物体检测）的挑战，并为此引入一个专门设计的数据集，以促进相关算法的开发与评估。
*   **⭐ 主要发现**: 本文的核心贡献是推出了MTevent数据集，这是一个专为高动态环境下6D姿态估计和运动物体检测而设计的多任务事件相机数据集。该数据集的创新之处在于其针对大检测距离和挑战性场景的覆盖，旨在弥补现有数据集在高速、高动态感知方面的不足。数据采集系统由立体事件相机和RGB相机组成，共捕获了75个场景，每个场景平均时长16秒，并包含了16种独特的物体，涵盖了各种具有挑战性的条件。MTevent数据集为解决高速移动机器人感知中的关键问题提供了宝贵的资源，尤其是在传统RGB相机受限的场景下。它将极大地促进基于事件相机的6D姿态估计和运动物体检测算法的开发与评估，对推动高速机器人感知领域的发展具有重要意义。

---
### [AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition](http://arxiv.org/abs/2505.11232v2)
**📅 发布日期**: 2025-05-16

*   **👥 作者**: Haiyu Li, Charith Abhayaratne
*   **🎯 研究目的**: 事件相机以高时间分辨率捕捉亮度变化，但其固有特性导致在获取物体结构信息的同时，会产生大量冗余和噪声数据。在基于事件的物体识别中，核心挑战在于如何在有效去除这些噪声的同时，不损失关键的时空信息。本研究旨在解决这一挑战，提出一种自适应图基去噪框架，以实现事件相机数据的联合去噪和目标识别。
*   **⭐ 主要发现**: 为了解决事件相机数据去噪和目标识别的挑战，本文提出了一种名为AW-GATCN（自适应加权图注意力卷积网络）的自适应图基噪声数据去除框架。该框架的核心创新在于整合了以下关键技术：
    *   基于归一化密度分析的自适应事件分割，以更精确地划分事件数据。
    *   一种多因素边缘加权机制，用于更有效地捕获事件之间的复杂关系。
    *   以及自适应图基去噪策略，能够根据数据特性灵活调整去噪过程。
    这些创新显著增强了时空信息的整合能力，从而能够有效过滤噪声，同时最大限度地保留事件数据中关键的结构特征，为事件相机数据的联合去噪和目标识别提供了新的有效解决方案。

---
### [[Maximizing Asynchronicity in Event-based Neural Networks]](http://arxiv.org/abs/2505.11165v1)
<!-- 2025-05-16 -->
**📅 发布日期**: 2025-05-16

*   **👥 作者**: Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang
*   **🎯 研究目的**: 事件相机以其高时间分辨率、低延迟和最小冗余的特点提供视觉数据，但其异步、稀疏的序列性质对传统的基于张量的机器学习（ML）方法构成了挑战。尽管近期出现的异步到同步（A2S）范式旨在通过异步编码事件来生成用于ML管道的学习表示，但现有的A2S方法在表示的表达能力和泛化性方面往往不如密集、同步的方法。本研究旨在解决这一问题，开发一种新的A2S框架，以生成更具表达能力和泛化性的事件表示。
*   **⭐ 主要发现**: 本文引入了EVA（EVent Asynchronous representation learning），这是一个新颖的A2S框架，能够生成高度富有表达能力和泛化性的逐事件表示。EVA的创新之处在于，它受到事件与语言之间类比的启发，创造性地借鉴了语言建模（特别是线性注意力机制）的最新进展来处理事件数据。通过这种独特的方法，EVA成功克服了现有A2S方法在表示能力上的局限性，为事件相机数据的机器学习处理提供了更高效和强大的解决方案。

---
### [Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow](http://arxiv.org/abs/2505.11116v1)
**📅 发布日期**: 2025-05-16

*   **👥 作者**: Liam Boyle, Jonas Kühne, Nicolas Baumann, Niklas Bastuck, Michele Magno
*   **🎯 研究目的**: 移动机器人中准确的速度估计至关重要，尤其对于驾驶辅助系统和自动驾驶。然而，现有广泛使用的轮式里程计与惯性测量单元（IMU）数据融合方法，通常需要强假设（如无滑移转向）或复杂的车辆动力学模型，这些在湿滑路面等不同环境条件下难以成立。因此，本文旨在提出一种新的速度估计方法，该方法能够摆脱对车轮与地面牵引力假设的依赖，以提高在复杂环境下的速度估计鲁棒性。
*   **⭐ 主要发现**: 本文介绍了一种创新的速度估计方法，该方法通过利用平面运动学原理，并结合垂直指向地面的事件相机（event camera）产生的光流信息，实现了与车轮-地面牵引力假设的解耦。事件相机具有异步微秒级延迟和高动态范围的特点，使其对运动模糊具有高度鲁棒性，而运动模糊是传统相机在快速运动中常见的挑战。这种基于事件相机光流的方法，有望在传统速度估计方法受限的复杂或恶劣环境（如湿滑路面）下，为高速移动机器人提供更准确、更可靠的速度信息。

---
### [[Contactless Cardiac Pulse Monitoring Using Event Cameras]](http://arxiv.org/abs/2505.09529v2)
**📅 发布日期**: 2025-05-14

*   **👥 作者**: Mohamed Moustafa, Joseph Lemley, Peter Corcoran
*   **🎯 研究目的**:
    事件相机作为一种新型技术，以极低的延迟和功耗记录场景信息，并能以比传统相机更高的动态范围和时间分辨率捕获像素级光强度变化。本研究旨在探索利用事件相机记录的面部数据，通过监督式卷积神经网络（CNN）模型，实现对个体心脏脉搏信号的非接触式重建。核心目标是验证事件相机在生理信号监测领域的潜力，为开发更高效、非侵入性的心率监测方法提供技术基础。
*   **⭐ 主要发现**:
    本研究成功开发并训练了一个端到端的监督式卷积神经网络模型，该模型能够从事件流的二维表示中提取心脏信号。模型的性能通过计算心率的准确性进行评估。实验结果初步证实，利用事件相机进行生理心脏信号的非接触式提取是可行的。这项研究为事件相机在远程健康监测、智能穿戴设备以及需要高时间分辨率和低功耗的生理信号检测应用中开辟了新的可能性。
### [[A Survey of 3D Reconstruction with Event Cameras]](http://arxiv.org/abs/2505.08438v2)
**📅 发布日期**: 2025-05-13

*   **👥 作者**: Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu, Weidong Cai
*   **🎯 研究目的**: 事件相机作为一种新型视觉传感器，在三维重建领域迅速崛起，能够异步捕捉像素级的亮度变化。与传统基于帧的相机相比，事件相机生成稀疏但时间密度高的数据流，使其在高速运动、低光照和极端动态范围等挑战性条件下，也能实现鲁棒且精确的三维重建。这些独特能力为自动驾驶、机器人、空中导航和沉浸式虚拟现实等多个领域的变革性应用带来了巨大潜力。本论文（作为一篇综述）旨在对基于事件相机的三维重建领域进行首次全面且专属的系统性回顾，以期为该领域的研究者提供清晰的概览和方向。
*   **⭐ 主要发现**: 本论文是首篇专门针对基于事件相机的三维重建领域进行的全面综述。其核心贡献在于系统地分类和回顾了现有方法，这有助于研究人员清晰地理解该领域的最新进展、技术挑战以及未来的发展方向。通过对输入数据、重建方法和应用场景等方面的归类，本综述为事件相机在三维重建领域的应用提供了一个结构化的知识框架，为后续研究奠定了坚实基础，并有望推动该技术在实际应用中的进一步发展。

---
### [[EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation]](http://arxiv.org/abs/2505.08235v1)
**📅 发布日期**: 2025-05-13

*   **👥 作者**: Hanle Zheng, Xujie Han, Zegang Peng, Shangbin Zhang, Guangxun Du, Zhuo Zou, Xilin Wang, Jibin Wu, Hao Guo, Lei Deng
*   **🎯 研究目的**: 视频帧插值（VFI）是计算机视觉领域一项基础但极具挑战性的任务，尤其是在处理大运动、遮挡和光照变化等复杂场景时。事件相机（event camera）的最新进展为解决这些挑战带来了新的机遇。尽管现有基于事件的VFI方法通过利用光流等手工设计的中间表示，成功恢复了大规模和复杂的运动，但这种对显式运动建模的依赖往往会损害在微小运动场景下的高保真图像重建能力。与此同时，扩散模型通过去噪过程重建帧，无需显式运动估计或扭曲操作，为VFI提供了一种有前景的替代方案。本研究旨在结合事件相机和扩散模型的优势，开发一个统一且高效的扩散模型框架，以克服现有方法的局限性，实现对各种运动场景下的高保真视频帧插值。
*   **⭐ 主要发现**: 本文的核心贡献在于提出了“EventDiff”，一个统一且高效的基于事件的视频帧插值扩散模型框架。EventDiff创新性地将事件相机所捕捉的异步、高动态范围数据与扩散模型的强大生成能力相结合，从而避免了传统基于事件的VFI方法中对显式运动估计或图像扭曲操作的依赖。这一方法有望解决现有方法在微小运动场景下高保真重建能力不足的问题，同时保持其在大运动场景下的出色表现。通过利用扩散模型的隐式建模能力，EventDiff能够更鲁棒、更精确地插值视频帧，预计将显著提升VFI在复杂环境（如大运动、遮挡和光照变化）下的性能，为高质量视频处理和计算机视觉领域带来新的突破。

---
### [[Asynchronous Multi-Object Tracking with an Event Camera]](http://arxiv.org/abs/2505.08126v1)
**📅 发布日期**: 2025-05-12

*   **👥 作者**: Angus Apps, Ziwei Wang, Vladimir Perejogin, Timothy Molloy, Robert Mahony
*   **🎯 研究目的**: 该研究旨在利用事件相机（event camera）的独特优势，即其低延迟输出、高时间分辨率和高动态范围，使其成为机器人在高动态环境中检测和跟踪物体的理想传感器。论文的核心目标是提出一种名为异步事件多目标跟踪（AEMOT）的算法，该算法能够通过异步处理单个原始事件，实现对多个目标的有效检测和跟踪，从而解决传统相机在极端动态环境下目标跟踪的挑战。
*   **⭐ 主要发现**:
    *   **核心算法**: 论文提出并详细介绍了异步事件多目标跟踪（AEMOT）算法，该算法能够通过异步处理来自事件相机的单个原始事件，实现对多个目标的检测和跟踪。
    *   **特征检测创新**: AEMOT算法通过构建一个新颖的“活跃流方向场”（Field of Active Flow Directions），该场源自“活跃事件表面”（Surface of Active Events），从而识别出具有一致光流的区域，进而检测到显著的事件斑点特征。
    *   **跟踪机制**: 检测到的特征被作为候选对象，利用近期提出的异步事件斑点（AEB）跟踪器进行跟踪，并为每个候选对象构建小的强度补丁。
    *   **验证阶段**: 算法包含一个新颖的学习验证阶段，用于对候选对象进行提升（promote）或丢弃（discard），这表明其具备更强的鲁棒性和准确性。
    *   **应用潜力**: AEMOT算法充分利用了事件相机在高动态环境中的固有优势，为机器人视觉、自主导航和监控等领域的多目标跟踪提供了高效且鲁棒的解决方案。

---
### [[Hybrid Spiking Vision Transformer for Object Detection with Event Cameras]](http://arxiv.org/abs/2505.07715v1)
**📅 发布日期**: 2025-05-12

*   **👥 作者**: Qi Xu, Jie Deng, Jiangrong Shen, Biwu Chen, Huajin Tang, Gang Pan
*   **🎯 研究目的**: 基于事件的物体检测因其高时间分辨率、宽动态范围和异步地址事件表示等独特优势而日益受到关注。脉冲神经网络（SNNs）作为一种有前景的方法，凭借其低能耗和丰富的时空动态特性，在这一领域展现出巨大潜力。本研究旨在进一步提升基于事件的物体检测性能，为此提出了一种新颖的混合脉冲视觉Transformer（HsVT）模型，以期更好地利用事件数据的时空特性。
*   **⭐ 主要发现**: 本文提出了一种创新的混合脉冲视觉Transformer（HsVT）模型，旨在显著提升基于事件的物体检测能力。HsVT模型的创新之处在于其独特的混合架构，该架构集成了两个关键模块：
    1.  **空间特征提取模块**：专门用于捕获事件数据中的局部和全局空间特征。
    2.  **时间特征提取模块**：用于有效建模事件序列中的时间依赖性和长期模式。
    这种巧妙的结合使得HsVT模型能够全面地捕捉事件数据的时空特征，从而显著增强其处理复杂事件流和进行精确物体检测的能力。

---
### [[Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells]](http://arxiv.org/abs/2505.07661v1)
**📅 发布日期**: 2025-05-12

*   **👥 作者**: Elad Yoshai, Dana Yagoda-Aharoni, Eden Dotan, Natan T. Shaked
*   **🎯 研究目的**: 传统卷积神经网络（CNNs）在图像分类中通常处理整个图像，无论信息密度如何，这导致计算效率低下，并可能将计算资源浪费在不相关的特征上。本文旨在解决这一问题，提出一种名为SparseAttnNet的新型分层注意力驱动框架，用于高效的图像分类，特别是针对生物细胞的分类任务。其核心目标是通过自适应地选择和处理图像中最具信息量的像素，从而显著提高计算效率并避免对无关特征的关注。
*   **⭐ 主要发现**: 本文提出了SparseAttnNet，一个创新的分层注意力驱动框架，用于高效的图像分类。其核心创新在于引入了一种动态像素选择机制：该机制利用模型下游层中由精细多头注意力提炼出的粗粒度注意力，自适应地识别并提取图像中最显著的 `k` 个像素。其中，`k` 的值是根据训练过程中损失收敛趋势自适应学习的。一旦选定这些“Top-k”像素，模型将仅处理这些精选像素（并将其嵌入为“词”），从而显著减少了不必要的数据处理，提高了计算效率。这一方法解决了传统CNN处理整个图像导致的计算冗余和对无关特征关注的问题，有望在生物细胞等特定图像分类任务中实现更高效、更精准的分类。
### [[Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs]](http://arxiv.org/abs/2505.07556v1)
<!-- 2025-05-12 -->
**📅 发布日期**: 2025-05-12

*   **👥 作者**: Kamil Jeziorek, Tomasz Kryjak
*   **🎯 研究目的**: 事件相机因其微秒级时间分辨率、在不同光照条件下的鲁棒性以及低功耗等优势，相较于传统基于帧的传感器具有显著优点。然而，有效处理其稀疏、异步的事件流仍然是一个挑战。现有方法主要分为两类：第一类是使用神经网络模型（如脉冲神经网络或图卷积神经网络）直接处理事件数据，但这通常以牺牲性能为代价；第二类是将事件转换为密集表示，通过手工设计的聚合函数来提升精度，但这会牺牲时间（摘要在此处截断，但暗示了某种权衡）。本研究旨在解决这些现有方法的局限性，探索一种新的事件表示方法，以在SoC FPGA上实现准确、实时的感知。
*   **⭐ 主要发现**: 提供的摘要不完整，因此无法详细阐述论文的核心贡献、具体的实验结果和理论突破。然而，根据论文标题“自监督事件表示”，该研究很可能提出了一种新颖的自监督学习方法，用于生成事件表示。这种方法旨在克服现有事件处理方案的局限性——即直接处理事件数据时性能的折衷，以及通过手工聚合函数转换为密集表示时可能牺牲的时间分辨率或其他方面的权衡。其核心目标是在SoC FPGA等嵌入式硬件平台上实现事件数据的准确、实时处理，从而为事件相机在实际应用中的部署提供更高效和鲁棒的解决方案。

---
### EDmamba: A Simple yet Effective Event Denoising Method with State Space Model
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-05-08

*   **👥 作者**: Ciyu Ruan, Zihang Gong, Ruishan Guo, Jingao Xu, Xinlei Chen
*   **🎯 研究目的**: 事件相机因其高时间分辨率、高动态范围和低功耗而在高速视觉领域表现出色。然而，作为动态视觉传感器，其输出固有的噪声是一个关键问题，高效的去噪对于保持其超低延迟和实时处理能力至关重要。现有的事件去噪方法面临一个两难困境：计算密集型方法会牺牲传感器的告诉优势，而轻量级方法则往往在不同噪声水平下缺乏鲁棒性。为了解决这一问题，本研究旨在提出一种基于状态空间模型（SSM）的新型事件去噪框架，以期在保证计算效率的同时，提升去噪的鲁棒性。
*   **⭐ 主要发现**: 本文提出了一种名为 EDmamba 的新型事件去噪框架，其核心创新在于利用状态空间模型（SSM）来处理事件数据。为了克服现有方法在计算效率和鲁棒性之间的矛盾，EDmamba 将事件表示为四维事件云（4D event clouds），并引入了一个粗粒度特征提取（Coarse Feature Extraction, CFE）模块。该模块能够从几何和极坐标等多个维度提取嵌入特征，从而实现高效且鲁棒的事件去噪。这种基于 SSM 的方法有望在保持事件相机低延迟和实时处理能力的同时，有效去除噪声，为高速视觉应用提供更清晰、可靠的数据。

---
### [[PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining]](http://arxiv.org/abs/2505.05307v1)
<!-- 2025-05-08 -->
**📅 发布日期**: 2025-05-08

*   **👥 作者**: Ciyu Ruan, Ruishan Guo, Zihang Gong, Jingao Xu, Wenhan Yang, Xinlei Chen
*   **🎯 研究目的**: 事件相机以其高时间分辨率和宽动态范围而闻名，但在雨天条件下会产生密集的噪声，严重影响其性能。现有事件相机去雨方法在时间精度、去雨效果和计算效率之间往往难以兼顾。本文旨在提出一种名为 PRE-Mamba 的新型点基事件相机去雨框架，旨在充分利用原始事件和雨的空时特性，从而解决现有方法的局限性，实现高效且高质量的事件相机去雨。
*   **⭐ 主要发现**: 本文提出了 PRE-Mamba 框架，这是一个新颖的点基事件相机去雨框架，旨在充分利用原始事件和雨的空时特性。其核心创新点和贡献包括：
    *   **4D 事件云表示**: 引入了一种创新的4D事件云表示方法，该方法集成了双时间尺度，以最大限度地保留事件数据固有的高时间精度。
    *   **空时解耦与融合模块 (STDF)**: 设计了一个独特的空时解耦与融合模块（STDF），通过实现时间信息和空间信息的浅层解耦与有效交互，显著增强了去雨能力。
    *   **多尺度状态空间模型 (MS3M)**: 引入了多尺度状态空间模型（MS3M），该模型能够深入捕捉雨水动态，进一步提升了去雨效果。
    *   通过这些创新组件的协同作用，PRE-Mamba 框架有望在保持高时间精度的同时，有效去除雨天噪声，并在去雨效果和计算效率之间取得更好的平衡，为事件相机在恶劣天气条件下的应用提供了新的解决方案。

---
### [[Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow]](http://arxiv.org/abs/2505.05089v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-05-08

*   **👥 作者**: Zuntao Liu, Hao Zhuang, Junjie Jiang, Yuhang Song, Zheng Fang
*   **🎯 研究目的**: 事件相机在捕获连续时空运动信息方面具有独特潜力，使其非常适合光流估计任务。然而，当前多数基于学习的事件相机光流方法存在两大局限：1) 它们通常采用基于帧的技术，未能充分利用事件固有的时空特性；2) 它们假设损失时间窗口内连续事件之间存在线性运动，这在处理长时间序列时会导致光流误差显著增加。本研究旨在解决这些问题，核心目标是开发一种新型的无监督事件相机光流网络，该网络能够充分利用事件数据中丰富的时空信息，并精确建模事件之间的非线性运动，从而在长时间序列中实现更准确的光流估计。
*   **⭐ 主要发现**: 本论文提出了一种名为 E-NMSTFlow 的新型无监督事件相机光流网络，专门针对长时间序列的光流估计进行了优化。核心创新点在于：1) 强调并利用了事件数据中丰富的时空信息，纠正了现有方法忽略事件时空特性的不足；2) 精确建模了事件之间的非线性运动，克服了传统方法假设线性运动导致在长时间序列中误差增大的问题。研究观察到，丰富的时空信息和准确的非线性运动对于事件相机光流估计至关重要。为此，E-NMSTFlow 引入了“时空运动特征”（Spatio-Temporal Motion Feature）的概念，以更好地捕捉事件的动态特性。通过这些创新，E-NMSTFlow 有望显著提高事件相机在复杂、长时间运动场景下的光流估计精度，为相关领域的应用提供更鲁棒的解决方案。

---
### [[TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion]](http://arxiv.org/abs/2505.03116v1)
**📅 发布日期**: 2025-05-06

*   **👥 作者**: Haoyue Liu, Jinghan Xu, Yi Chang, Hanyu Zhou, Haozhi Zhao, Lin Wang, Luxin Yan
*   **🎯 研究目的**: 视频帧插值（VFI）是计算机视觉中的一项重要任务。近年来，受生物启发事件相机引导的VFI方法因其高时间分辨率等优势，在性能和内存效率上超越了传统基于帧的方法。然而，现有事件相机VFI方法在处理场景中由运动方向和速度动态变化引起的非线性运动时面临巨大挑战。当前方法通过估计稀疏或稠密光流来利用事件数据，但由于事件提供的连续运动线索与图像的稠密空间信息在时间维度上不匹配，常常导致运动误差，进而降低VFI质量。本研究旨在解决这一关键问题，提升事件相机VFI在非线性运动场景下的插值质量。
*   **⭐ 主要发现**: 尽管摘要内容不完整，但从论文标题和现有信息可推断，本文的核心贡献在于提出了名为“TimeTracker”的新方法。该方法专注于事件驱动的连续点跟踪，旨在解决视频帧插值中非线性运动的挑战。论文指出，物体运动在空间上是连续的，这一发现可能构成了TimeTracker方法的基础，使其能够更有效地利用事件相机的连续运动线索，克服现有光流估计方法中事件与图像信息在时间维度上不对齐的问题。TimeTracker有望通过提供更精确、连续的运动轨迹，显著提升事件相机VFI在复杂非线性运动场景下的插值质量，为未来事件相机在计算机视觉领域的应用开辟新途径。

---
### [[DELTA: Dense Depth from Events and LiDAR using Transformer's Attention]](http://arxiv.org/abs/2505.02593v1)
**📅 发布日期**: 2025-05-05

*   **👥 作者**: Vincent Brebion, Julien Moreau, Franck Davoine
*   **🎯 研究目的**: 事件相机和激光雷达分别提供异步光线变化检测和稀疏但精确的深度信息，两者数据互补但形式不同。鉴于目前鲜有研究探索这两种模态的结合，本文旨在提出一种新颖的基于神经网络的方法，以融合事件和激光雷达数据，从而估计出高密度的深度图。
*   **⭐ 主要发现**: 本文提出名为DELTA的新型神经网络架构，其核心创新在于巧妙地利用自注意力（self-attention）和交叉注意力（cross-attention）机制，以建模事件数据和激光雷达数据内部及其相互之间的空间与时间关系，从而实现数据的高效融合。经过全面的评估，实验结果表明DELTA在基于事件的深度估计问题上取得了显著进展，不仅刷新了现有技术水平（State-of-the-Art, SOTA），而且在近距离场景下，相较于以往的最佳方法，能够将深度估计误差降低高达四倍，展现了其在提升深度感知精度方面的巨大潜力。

---
### [[Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation]](http://arxiv.org/abs/2505.01548v1)
**📅 发布日期**: 2025-05-02

*   **👥 作者**: Zhen Yao, Xiaowen Ying, Mooi Choo Chuah
*   **🎯 研究目的**: 事件相机因其捕捉运动动态的能力，在计算机视觉任务中展现出巨大潜力。然而，RGB与事件数据融合面临固有的三大错位挑战：时间错位、空间错位和模态错位。现有体素网格等事件表示方法忽视了连续事件窗口间的时间关联性，且其对异步稀疏事件的简单累积方式与RGB模态的同步密集特性不兼容。本研究旨在解决这些挑战，提出一种新的事件表示方法，以克服现有方法的局限性，最终提升RGB-事件融合在语义分割等任务中的性能。
*   **⭐ 主要发现**: 为了解决RGB-事件融合中的固有错位问题以及现有事件表示的不足，本研究提出了一种新颖的事件表示方法——**运动增强事件张量（Motion-enhanced Event Tensor, MET）**。MET通过利用密集的**光流**和**事件时间特征**，将稀疏的事件体素转换为密集且时间连贯的形式。这种创新性方法有效地解决了事件数据的稀疏性和时间不连贯性问题，使其与同步密集的RGB模态更加兼容。此外，论文还引入了一种“频率感知双向……”（摘要在此处被截断，但MET是核心贡献）。这些创新有望显著提升RGB-事件语义分割的性能，为多模态融合领域提供新的思路和解决方案。

---
### [[From Events to Enhancement: A Survey on Event-Based Imaging Technologies]](http://arxiv.org/abs/2505.05488v1)
**📅 发布日期**: 2025-04-30

*   **👥 作者**: Yunfan Lu, Xiaogang Xu, Pengteng Li, Yusheng Wang, Yi Cui, Huizai Yao, Hui Xiong
*   **🎯 研究目的**: 事件相机凭借其高动态范围和低延迟的优势，已成为成像领域颠覆性技术。尽管利用这些优势进行各种成像任务的研究日益增多，但目前仍缺乏对最新进展和挑战的全面综述。这限制了人们对如何在通用成像应用中利用事件数据的广泛理解。因此，本研究旨在填补这一空白，提供一个全面的调查，以促进对事件成像技术的理解和应用。
*   **⭐ 主要发现**: 本综述系统地阐述了事件成像技术，其主要贡献和发现包括：
    1.  **基础理论与传感器特性**: 论文首先介绍了事件相机的物理模型和不同事件传感器的特性，为理解其工作原理奠定了基础。
    2.  **图像/视频增强**: 重点探讨了事件数据在图像和视频增强任务中的最新进展及其相互作用，展示了事件相机如何提升传统成像的性能。
    3.  **高级光信息捕获**: 进一步探索了利用事件数据捕获更丰富光信息的高级任务，例如光场估计、多视角生成和光度学应用。
    4.  **挑战与展望**: 最后，论文讨论了当前面临的新挑战和未来的开放性问题，为该领域的研究指明了方向。
    通过提供这一全面的综述，该研究旨在促进研究人员和开发者更深入地理解事件成像技术，并推动其在更广泛的通用成像应用中的开发和利用。

---
### [[A Real-Time Event-Based Normal Flow Estimator]](http://arxiv.org/abs/2504.19417v1)
**📅 发布日期**: 2025-04-28

*   **👥 作者**: Dehao Yuan, Cornelia Fermüller
*   **🎯 研究目的**: 本文旨在提出一种实时、异步、基于事件的法向流（Normal Flow）估计器。该研究的核心目标是优化现有“直接从事件邻域学习法向流”算法的实现，以解决其在处理大量事件时面临的二次时间复杂度问题，从而实现更高效、更实时的法向流估计。
*   **⭐ 主要发现**: 论文的核心贡献在于提出了一种经过高度优化的实现方法，显著提升了基于事件的法向流估计器的效率和实时性。具体创新点在于：
    *   **问题识别**: 现有方法将事件切片视为3D点云，通过邻接矩阵与特征矩阵相乘来构建事件表示，导致计算复杂度与事件数量呈二次方关系，限制了实时应用。
    *   **创新方法**: 本文利用事件坐标为整数的特性，巧妙地将表示构建步骤重新表述为一种池化（pooling）操作。
    *   **性能提升**: 这种创新的池化操作在实现与传统邻接矩阵相同效果的同时，大幅降低了计算成本。
    *   **潜在影响**: 这种优化使得法向流估计能够以更低的计算开销实现真正的实时性能，为事件相机在高速、低延迟视觉感知应用中提供了更高效、更可行的解决方案。

---
### [[E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization]](http://arxiv.org/abs/2504.18521v1)
**📅 发布日期**: 2025-04-25

*   **👥 作者**: Shintaro Shiba, Quan Kong, Norimasa Kobori
*   **🎯 研究目的**: 本研究旨在解决当前在可见光通信（VLC）领域中，缺乏用于在各种真实世界场景下对事件相机进行信号解码和相机定位性能进行基准测试的公开数据集的问题。鉴于事件相机凭借其高时空分辨率，在利用调制LED进行光学通信中展现出巨大潜力，并且能够同时实现信号解码和相机相对于LED标记的定位，因此构建一个包含真实世界数据的基准数据集对于推动该领域的发展至关重要。
*   **⭐ 主要发现**: 本论文最主要的贡献是发布了首个公开的、用于事件相机可见光通信（VLC）和定位的真实世界数据集。该数据集的创新之处在于其包含了事件相机数据、帧相机数据，以及通过硬件触发器精确同步的地面真实姿态（ground-truth poses）。数据集设计考虑了多样性，涵盖了不同相机运动、多种灵敏度设置以及室内外不同场景亮度条件下的数据，极大地丰富了研究资源。此外，论文还提出了一种新颖的定位方法（具体细节在摘要中未完全展开）。这一数据集的发布将填补该领域在真实世界基准数据方面的空白，为研究人员开发和评估事件相机在VLC信号解码和相机定位方面的算法提供了宝贵的平台，有望加速相关技术的发展和应用。

---
### [Iterative Event-based Motion Segmentation by Variational Contrast Maximization](http://arxiv.org/abs/2504.18447v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-04-25

*   **👥 作者**: Ryo Yamaki, Shintaro Shiba, Guillermo Gallego, Yoshimitsu Aoki
*   **🎯 研究目的**: 事件相机因其对场景变化的响应特性，能够提供丰富的信号，非常适用于运动估计任务。然而，场景中任何视觉变化都会产生事件数据，因此将这些数据分类为不同的运动（即运动分割）至关重要。运动分割对于目标检测和视觉伺服等多种下游任务都具有重要意义。本文旨在解决事件数据中运动分类的挑战，提出一种新的迭代运动分割方法，以有效地区分场景中的不同运动，从而为后续应用提供清晰、准确的运动信息。
*   **⭐ 主要发现**: 本文提出了一种创新的迭代运动分割方法，该方法通过将事件数据分类为背景（例如，主导运动假设）和前景（独立运动残差），从而扩展了现有的对比度最大化（Contrast Maximization）框架。这项方法的核心创新在于其迭代处理机制以及对事件数据进行精细分类的能力。实验结果表明，所提出的方法在公共数据集和自录制数据集上都成功地对事件簇进行了分类，并生成了清晰、经过运动补偿的类边缘图像。这不仅验证了该方法在事件运动分割任务上的有效性，也预示着其在性能上能够达到或超越现有最先进（state-of-the-art）的水平，为基于事件相机的运动分析和感知任务提供了新的强大工具。

---
### [[Event-Based Eye Tracking. 2025 Event-based Vision Workshop]](http://arxiv.org/abs/2504.18249v1)
**📅 发布日期**: 2025-04-25

*   **👥 作者**: Qinyu Chen, Chang Gao, Min Liu, Daniele Perrone, Yan Ru Pei, Zuowen Wang, Zhuo Zou, Shihang Tan, Tao Han, Guorui Lu, Zhen Xu, Junyuan Ding, Ziteng Wang, Zongwei Wu, Han Han, Yuliang Wu, Jinze Chen, Wei Zhai, Yang Cao, Zheng-jun Zha, Nuwan Bandara, Thivya Kandappu, Archan Misra, Xiaopeng Lin, Hongxiang Huang, Hongwei Ren, Bojun Cheng, Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, Tram T. Doan
*   **🎯 研究目的**: 本综述旨在为2025年CVPR事件相机视觉研讨会中举办的“2025事件相机眼动追踪挑战赛”提供回顾。该挑战赛的核心任务是通过处理事件相机记录的眼球运动数据来预测瞳孔中心。研究旨在总结挑战赛中排名靠前团队的创新方法，以期推动未来事件相机眼动追踪领域的研究进展。
*   **⭐ 主要发现**: 论文详细回顾并总结了在“2025事件相机眼动追踪挑战赛”中表现顶尖团队的创新方法。对于每种方法，论文报告了其准确性、模型大小和操作数量等关键指标，为评估和比较不同方案提供了量化依据。此外，本综述还从硬件设计的角度深入探讨了事件相机眼动追踪技术，为该领域的未来发展提供了全面的视角和潜在方向。

---
### [[BiasBench: A reproducible benchmark for tuning the biases of event cameras]](http://arxiv.org/abs/2504.18235v1)
**📅 发布日期**: 2025-04-25

*   **👥 作者**: Andreas Ziegler, David Joseph, Thomas Gossard, Emil Moldovan, Andreas Zell
*   **🎯 研究目的**: 事件相机作为一种仿生传感器，因其高时间分辨率、低延迟和高动态范围等优势，在计算机视觉和机器人领域得到日益广泛的应用。然而，与任何相机一样，事件相机的输出质量在很大程度上取决于其“偏置”（biases，即相机设置）的配置。与传统帧相机拥有先进的自动配置算法不同，事件相机在偏置调优方面缺乏此类工具。建立一个系统性的测试框架需要观察同一场景在不同偏置下的表现，但这极具挑战性，因为事件相机仅在有运动时才生成事件。本研究旨在解决这一空白，提出一个可复现的基准测试平台（BiasBench），以促进事件相机偏置的系统性调优和评估，从而提升其性能和应用潜力。
*   **⭐ 主要发现**: 本论文的核心贡献是引入了“BiasBench”——一个专为事件相机偏置调优而设计的可复现基准测试平台。该平台旨在填补当前事件相机缺乏自动化配置工具的空白，为研究人员和开发者提供一个系统性的框架来评估和优化事件相机的设置。通过提供一个标准化的测试环境，BiasBench有望克服事件相机在无运动时难以生成事件的测试挑战，从而促进对事件相机性能的深入理解和优化，最终提升其在各类应用中的输出质量和可靠性。
### [EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception](http://arxiv.org/abs/2504.16616v2)
**📅 发布日期**: 2025-04-23

*   **👥 作者**: Haosheng Chen, Lian Luo, Mengjingcheng Mo, Zhanjie Wu, Guobao Xiao, Ji Gan, Jiaxu Leng, Xinbo Gao
*   **🎯 研究目的**: 事件相机以其微秒级时间分辨率和高动态范围（HDR）特性，能够产生高速事件流，适用于多种感知任务。然而，当前基于图神经网络（GNN）的感知方法普遍存在局限性：它们倾向于在纯欧几里得空间中采用简单的成对连接机制，这导致它们难以捕捉事件流中的长距离依赖关系，并且无法有效表征非均匀分布事件流固有的层次结构。为此，本研究旨在提出一种新颖的方法，以克服现有GNN在处理事件流时面临的这些挑战，从而更有效地感知和理解高速事件数据。
*   **⭐ 主要发现**: 本论文提出了一种名为EHGCN的创新方法，开创性地在欧几里得空间和双曲空间中同时感知事件流，以解决现有方法在处理长距离依赖和事件流固有层次结构方面的不足。EHGCN的核心创新点包括：
    *   **欧几里得-双曲空间融合**: EHGCN是首个将欧几里得空间（擅长建模局部和短程关系）与双曲空间（更适合建模层次结构和长距离依赖）相结合，用于事件视觉感知的方法，旨在更全面、更有效地表征非均匀分布的事件数据。
    *   **运动感知图卷积网络（Motion-Aware GCN）**: 通过引入运动感知机制，EHGCN能够更智能地处理事件流，可能有助于更好地理解事件的动态特性和结构。
    *   **自适应采样策略**: 引入了一种自适应采样策略，能够动态调整采样率，从而在保留关键判别性事件的同时，有效抑制噪声或冗余信息，进一步提升感知效率和准确性。
    这些创新使得EHGCN能够更有效地捕捉事件流的复杂特性，有望在事件视觉感知任务中取得显著性能提升，为混合事件流感知领域提供了新的范式。

---
### [[SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields]](http://arxiv.org/abs/2504.16389v1)
**📅 发布日期**: 2025-04-23

*   **👥 作者**: Yuanjian Wang, Yufei Deng, Rong Xiao, Jiahao Fan, Chenwei Tang, Deng Xiong, Jiancheng Lv
*   **🎯 研究目的**: 事件相机作为一种新型的神经形态视觉传感器，因其异步捕捉对数亮度变化的特性，在低延迟、低功耗、低带宽和高动态范围方面具有显著优势，使其成为高速场景的理想选择。然而，利用事件数据重建几何一致且光度准确的3D表示仍然是一个根本性的挑战。现有基于事件的神经辐射场（NeRF）方法虽然部分解决了这些挑战，但由于早期激进的网络学习以及事件相机固有的噪声，仍存在持续的伪影。本研究旨在克服这些局限性，提出一种有效抑制伪影的新框架。
*   **⭐ 主要发现**: 为解决现有事件相机NeRF方法中持续存在的伪影问题，本文提出了一种名为SaENeRF的新型自监督框架。SaENeRF的核心创新在于其能够有效抑制由早期激进网络学习和事件相机固有噪声所引起的伪影。通过这种伪影抑制机制，SaENeRF有望实现更准确、更鲁棒的事件数据3D重建，从而提升基于事件的NeRF在复杂高速场景中的应用潜力。

---
### [[DERD-Net: Learning Depth from Event-based Ray Densities]](http://arxiv.org/abs/2504.15863v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-04-22

*   **👥 作者**: Diego de Oliveira Hitzges, Suman Ghosh, Guillermo Gallego
*   **🎯 研究目的**: 事件相机因其在高速和宽泛光照条件下检测无模糊3D边缘的能力，在多视角立体深度估计和同步定位与建图（SLAM）中展现出巨大潜力。然而，为传统相机设计的深度学习框架难以处理事件数据的异步、流式特性，因为它们的架构是为离散、图像式输入优化的。本研究旨在解决这一挑战，提出一个可扩展、灵活且适应性强的框架，用于事件相机在单目和立体设置下的像素级深度估计。
*   **⭐ 主要发现**: 本文提出了一种新颖的深度学习框架——DERD-Net，用于利用事件相机进行像素级深度估计。其核心创新在于，它将3D场景结构编码成“视差空间图像（DSIs）”，这些图像通过将事件数据基于已知相机姿态反投影到空间中，来表示光线的空间密度。该神经网络专门处理这些DSIs，从而实现了在单目和立体设置下可扩展、灵活且适应性强的像素级深度估计。这种方法克服了传统深度学习框架处理异步事件数据的局限性，为事件相机在深度感知领域的应用开辟了新途径。

---
### [[Event2Vec：通过向量空间表示直接处理神经形态事件]](http://arxiv.org/abs/2504.15371v1)
**📅 发布日期**: 2025-04-21

*   **👥 作者**: Wei Fang, Priyadarshini Panda
*   **🎯 研究目的**: 神经形态事件相机在时间分辨率、能效和动态范围方面相较于传统相机具有压倒性优势。然而，事件相机输出的是异步、稀疏和不规则的事件数据，这使得它们与主流的计算机视觉和深度学习方法不兼容。尽管已提出多种方法来解决此问题，但通常需要漫长的预处理过程、牺牲时间分辨率或无法兼容大规模并行计算。本研究旨在解决这一核心挑战，受“词到向量”（Word2Vec）巨大成功的启发，提出一种新的事件数据表示方法，使其能够被深度学习模型直接、高效地处理，从而充分发挥事件相机的潜力。
*   **⭐ 主要发现**: 本文的核心贡献是首次提出了“事件到向量”（Event2Vec）表示方法。作者通过总结事件与词语之间的相似性，并借鉴“词到向量”的理念，构建了Event2Vec，实现了对神经形态事件的直接处理。该方法在ASL-DVS数据集的分类任务上进行了验证，结果显示出令人印象深刻的参数效率、准确性和处理速度。这一创新性表示方法为神经形态事件数据的处理开辟了新途径，有望显著提升事件相机在计算机视觉和深度学习应用中的兼容性和性能，推动相关领域的发展。

---
### [[Zebrafish Counting Using Event Stream Data]](http://arxiv.org/abs/2504.13692v1)
<!-- 2025-04-18 -->
**📅 发布日期**: 2025-04-18

*   **👥 作者**: Qianghua Chen, Huiyu Wang, Li Ming, Ying Zhao
*   **🎯 研究目的**: 斑马鱼因与人类基因高度同源而被广泛用作生物医学研究的模型生物。在医学实验室中，斑马鱼计数是一项日常任务。然而，由于斑马鱼体型微小，手动目视计数极具挑战性，而现有计数方法要么不适用于小型鱼类，要么存在诸多局限性。本研究旨在提出一种基于事件流数据的斑马鱼计数算法，以克服现有方法的不足，提高计数精度。
*   **⭐ 主要发现**: 本文提出了一种新颖的斑马鱼计数算法。该方法首先利用事件相机进行数据采集，随后进行相机校准和图像融合。关键创新点在于利用轨迹信息显著提高了计数精度。最终，通过对经验周期内的计数结果进行平均并四舍五入得到最终结果。该算法为医学实验室提供了一种更高效、更准确的斑马鱼计数解决方案，有望取代耗时且易出错的手动计数方法。

---

### [[Neural Ganglion Sensors: Learning Task-specific Event Cameras Inspired by the Neural Circuit of the Human Retina]](http://arxiv.org/abs/2504.13457v1)
<!-- 2025-04-18 -->
**📅 发布日期**: 2025-04-18

*   **👥 作者**: Haley M. So, Gordon Wetzstein
*   **🎯 研究目的**: 受人眼神经元高效脉冲机制的启发，事件相机通过发射异步、逐像素的强度变化而非传统的固定帧率图像，实现了高时间分辨率和最小的功耗与带宽需求。然而，与人眼视网膜神经节细胞（RGCs）不同，传统事件相机在决定触发哪些事件时，并未利用局部空间上下文信息。此外，人眼包含约20种不同类型的RGCs并行工作，每种都适应不同的特征或条件。本研究旨在受这种生物学设计的启发，扩展传统事件相机，使其能够学习任务特定的事件生成机制，以更有效地提取时空特征。
*   **⭐ 主要发现**: 本文引入了“神经节传感器”（Neural Ganglion Sensors），这是一种对传统事件相机的扩展。该传感器借鉴了人眼视网膜神经节细胞的设计，允许事件相机学习任务特定的事件触发机制。通过整合来自多个光感受器的信号并在感受野内提取时空特征，神经节传感器能够更智能地决定何时以及如何触发事件。这种方法有望提高事件相机在特定任务中的数据效率和性能，使其更接近生物视觉系统的复杂性和适应性。

---

### [[CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework]](http://arxiv.org/abs/2504.12576v1)
<!-- 2025-04-17 -->
**📅 发布日期**: 2025-04-17

*   **👥 作者**: Wentao Wu, Xiao Wang, Chenglong Li, Bo Jiang, Jin Tang, Bin Luo, Qi Liu
*   **🎯 研究目的**: 事件相机因其高动态范围、高时间分辨率、低功耗和低延迟等优势，近年来受到越来越多的关注。尽管一些研究者已开始直接在事件数据上探索预训练，但这些努力往往未能与RGB帧建立强连接，限制了它们在多模态融合场景中的适用性。本研究旨在解决这些问题，提出一个统一的预训练框架，以支持RGB-事件感知，并为基于事件和RGB-事件融合的下游任务提供强大的支持。
*   **⭐ 主要发现**: 本文提出了CM3AE预训练框架，该框架能够接受RGB图像、事件图像和事件体素等多种模态/视图数据作为输入，从而为事件基和RGB-事件融合的下游任务提供鲁棒支持。具体而言，CM3AE设计了一个多模态融合重建模块，能够重建不同模态的数据，从而在预训练阶段建立RGB帧与事件数据之间的强连接。这一创新框架显著提升了事件相机在多模态融合场景中的应用潜力，为未来更复杂的感知任务奠定了基础。

---

### [[Event Quality Score (EQS): Assessing the Realism of Simulated Event Camera Streams via Distances in Latent Space]](http://arxiv.org/abs/2504.12515v2)
<!-- 2025-04-16 -->
**📅 发布日期**: 2025-04-16

*   **👥 作者**: Kaustav Chanda, Aayush Atul Verma, Arpitsinh Vaghela, Yezhou Yang, Bharatesh Chakravarthi
*   **🎯 研究目的**: 事件相机以其低延迟、高动态范围和事件的异步性，预示着视觉传感的范式转变。然而，高质量标注数据集的稀缺性阻碍了它们在深度学习驱动的计算机视觉中的广泛应用。为了缓解这一问题，已提出多种模拟器来生成用于训练模型合成事件数据。但事件相机与传统帧相机根本不同的传感器设计，给精确模拟带来了挑战，导致大多数模拟数据未能模仿真实事件相机捕获的数据。本研究旨在提出一种新的质量度量标准，以评估模拟事件流的真实性。
*   **⭐ 主要发现**: 本文受现有使用深度特征进行图像比较工作的启发，引入了事件质量分数（Event Quality Score, EQS）。EQS是一种利用活动感知特征距离的质量度量标准，用于评估模拟事件相机流的真实性。通过在潜在空间中测量模拟数据与真实数据之间的距离，EQS能够量化模拟数据的逼真程度。这一指标的提出，为事件相机模拟器提供了一个客观的评估工具，有助于开发出更真实、更高质量的合成事件数据集，从而加速事件相机在计算机视觉领域的应用和发展。

---

### [[Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera]](http://arxiv.org/abs/2504.10984v2)
<!-- 2025-04-15 -->
**📅 发布日期**: 2025-04-15

*   **👥 作者**: Sami Arja, Nimrod Kruger, Alexandre Marcireau, Nicholas Owen Ralph, Saeed Afshar, Gregory Cohen
*   **🎯 研究目的**: 头足类动物尽管只有一种感光细胞，却能表现出独特的颜色辨别能力，这依赖于其眼部光学系统和瞳孔形状引起的色差来感知光谱信息。本研究旨在受这种生物机制的启发，设计一种结合球形透镜和事件相机的光谱成像系统，以实现单色事件相机的颜色视觉能力。
*   **⭐ 主要发现**: 本文提出了一种受头足类动物启发的光谱成像系统，该系统将球形透镜与事件相机相结合。通过一个电动系统来移动焦距位置，模仿头足类动物的自适应晶状体运动，该方法能够在可见光和近红外光谱范围内实现波长相关的聚焦，从而使事件相机成为一个光谱传感器。研究表征了色差效应，并使用事件基和传统帧基传感器进行了验证，证明了这种生物启发式方法在单色事件相机上实现颜色感知和光谱成像的有效性。这一创新为事件相机在多光谱成像领域的应用开辟了新途径。

---

### [[Perturbed State Space Feature Encoders for Optical Flow with Event Cameras]](http://arxiv.org/abs/2504.10669v1)
<!-- 2025-04-14 -->
**📅 发布日期**: 2025-04-14

*   **👥 作者**: Gokul Raju Govinda Raju, Nikola Zubić, Marco Cannici, Davide Scaramuzza
*   **🎯 研究目的**: 事件相机以其运动响应特性，在光流估计方面比传统相机具有显著优势。尽管深度学习已改进了传统方法，但当前用于事件基光流的神经网络仍面临时空推理的局限性。本研究旨在解决这些挑战，提出一种新的特征编码器，以提高事件相机多帧光流估计的性能。
*   **⭐ 主要发现**: 本文提出了扰动状态空间特征编码器（Perturbed State Space Feature Encoders, P-SSE），用于事件相机的多帧光流估计。P-SSE能够自适应地处理具有大感受野的时空特征，类似于基于Transformer的方法，同时保持状态空间模型（SSMs）的线性计算复杂度。该模型实现最先进性能的关键创新在于其应用于状态动力学矩阵的扰动技术。实验结果表明，P-SSE在事件基光流估计任务中取得了显著的性能提升，有效克服了现有方法在时空推理方面的局限性。

---

### [[RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework]](http://arxiv.org/abs/2504.10018v1)
<!-- 2025-04-14 -->
**📅 发布日期**: 2025-04-14

*   **👥 作者**: Xiao Wang, Haiyang Wang, Shiao Wang, Qiang Chen, Jiandong Jin, Haoyu Song, Bo Jiang, Chenglong Li
*   **🎯 研究目的**: 现有行人属性识别方法普遍基于RGB帧相机开发，但受限于RGB相机对光照条件敏感、易受运动模糊影响等问题，其性能受到限制。此外，当前属性识别主要关注行人外貌和衣着，缺乏对情感维度的探索。本研究旨在借鉴事件相机在低光、高速和低功耗方面的优势，重新审视这些问题，并提出一种新颖的多模态RGB-事件属性识别任务。
*   **⭐ 主要发现**: 本文提出了首个大规模多模态行人属性识别数据集EventPAR，包含10万对RGB-事件图像，旨在弥补现有数据集的不足。此外，还提出了一个非对称RWKV融合框架，用于有效融合RGB和事件数据。该框架能够充分利用两种模态的互补信息，在低光和高速场景下实现更鲁棒的行人属性识别。这项工作不仅为多模态行人属性识别提供了宝贵的基准数据集，也为未来该领域的研究提供了新的融合范式。

---

### [[EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting]](http://arxiv.org/abs/2504.10012v1)
<!-- 2025-04-14 -->
**📅 发布日期**: 2025-04-14

*   **👥 作者**: Yufei Deng, Yuanjian Wang, Rong Xiao, Chenwei Tang, Jizhe Zhou, Jiahao Fan, Deng Xiong, Jiancheng Lv, Huajin Tang
*   **🎯 研究目的**: 尽管3D高斯溅射（3D-GS）在真实感新视图合成方面表现出色，但其性能在运动模糊情况下会下降。在快速运动或低光照条件下，现有基于RGB的去模糊方法难以建模曝光期间的相机姿态和辐射变化，从而降低了重建精度。事件相机能够捕获曝光期间连续的亮度变化，可有效辅助建模运动模糊并提高重建质量。本研究旨在利用事件相机，从事件流和严重模糊的图像中重建清晰的3D高斯。
*   **⭐ 主要发现**: 本文提出了事件驱动的捆绑调整去模糊高斯溅射（EBAD-Gaussian）方法，该方法能够从事件流和严重模糊的图像中重建清晰的3D高斯。该方法在恢复曝光时间内的相机运动轨迹的同时，联合学习这些高斯参数。具体而言，EBAD-Gaussian通过有效利用事件相机提供的高时间分辨率运动信息，解决了传统RGB图像在运动模糊场景下的重建难题。实验结果表明，EBAD-Gaussian在运动模糊场景下的3D重建和新视图合成方面取得了显著的性能提升，为实时、高质量的3D重建提供了新途径。

---

### [[Low-Light Image Enhancement using Event-Based Illumination Estimation]](http://arxiv.org/abs/2504.09379v1)
<!-- 2025-04-13 -->
**📅 发布日期**: 2025-04-13

*   **👥 作者**: Lei Sun, Yuhan Bao, Jiajun Zhai, Jingyun Liang, Yulun Zhang, Kaiwei Wang, Danda Pani Paudel, Luc Van Gool
*   **🎯 研究目的**: 低光图像增强（LLIE）旨在提高在光照不足环境下捕获图像的可见性。现有基于事件的解决方案主要利用运动触发的“运动事件”来增强边缘纹理，而事件相机的高动态范围和出色的低光响应能力则在很大程度上未被充分探索。本研究旨在开辟一条新途径，从使用“时间映射”事件估计光照的角度出发，将通过透射率调制触发的事件时间戳转换为亮度值，以实现更有效的低光图像增强。
*   **⭐ 主要发现**: 本文提出了一种利用“时间映射”事件进行光照估计的新方法，从而实现低光图像增强。通过将透射率调制触发的事件时间戳转换为亮度值，该方法能够获得细粒度的光照线索。这些光照线索通过提出的“光照辅助反射率分解与增强网络”（Illumination-aided Reflectance Decomposition and Enhancement Network）促进了低光图像中反射率分量的更有效分解和增强。这项工作首次利用事件相机进行全局光照估计，为低光图像增强提供了一种新颖且高效的解决方案，充分利用了事件相机在低光环境下的独特优势。

---

### [[Hardware, Algorithms, and Applications of the Neuromorphic Vision Sensor: a Review]](http://arxiv.org/abs/2504.08588v1)
<!-- 2025-04-11 -->
**📅 发布日期**: 2025-04-11

*   **👥 作者**: Claudio Cimarelli, Jose Andres Millan-Romera, Holger Voos, Jose Luis Sanchez-Lopez
*   **🎯 研究目的**: 神经形态或事件相机代表了经典视觉传感方法的一次变革，它将检测到的瞬时逐像素光照变化编码为异步的事件数据流。与标准相机相比，其新颖之处在于从固定时间间隔捕获完整图像帧转变为稀疏数据格式，这以其独特的特性，在各种应用中提供了潜在的改进。然而，这些优势的代价是需要重新发明算法程序或对其进行调整以有效处理新的数据格式。本综述旨在系统地审视神经形态视觉，涵盖其技术演变、硬件特性、算法处理方法及其在不同领域的应用。
*   **⭐ 主要发现**: 本综述系统地考察了神经形态视觉的三个主要维度。首先，它强调了神经形态相机从2014年到2024年的技术演变和独特的硬件特性。其次，深入探讨了处理事件数据流的算法方法，包括数据表示、特征提取、运动估计、重建和识别等。最后，综述了神经形态视觉传感器在机器人、自动驾驶、虚拟现实、医疗等多个领域的广泛应用，并讨论了其面临的挑战和未来的发展方向。这篇综述为研究人员和工程师提供了对神经形态视觉传感器全面而深入的理解，有助于推动该领域的研究和应用。

---

### [[Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset]](http://arxiv.org/abs/2504.05830v1)
<!-- 2025-04-08 -->
**📅 发布日期**: 2025-04-08

*   **👥 作者**: Shiao Wang, Xiao Wang, Bo Jiang, Lin Zhu, Guoqi Li, Yaowei Wang, Yonghong Tian, Jin Tang
*   **🎯 研究目的**: 人体活动识别（HAR）主要依赖传统RGB相机实现高性能活动识别。然而，在现实世界场景中，诸如光照不足和快速运动等挑战性因素不可避免地会降低RGB相机的性能。为了应对这些挑战，受生物学启发的事件相机提供了一种有前景的解决方案，以克服传统RGB相机的局限性。本研究旨在结合RGB和事件相机，重新思考人体活动识别任务，并解决现有数据集和模型在多模态融合方面的不足。
*   **⭐ 主要发现**: 本文提出了首个大规模多模态RGB-事件人体活动识别基准数据集HARDVS 2.0，包含了300种日常真实世界动作的107,646对配对视频，弥补了数据集空白。其次，提出了一种新颖的多模态热传导模型，该模型能够有效融合RGB和事件数据，充分利用两种模态的互补优势。实验结果表明，所提出的模型在HARDVS 2.0数据集上取得了显著的性能提升，尤其是在光照不足和快速运动等挑战性场景下，展现了多模态融合在人体活动识别领域的巨大潜力。

---

### [[Inter-event Interval Microscopy for Event Cameras]](http://arxiv.org/abs/2504.04924v3)
<!-- 2025-04-07 -->
**📅 发布日期**: 2025-04-07

*   **👥 作者**: Changqing Su, Yanqin Chen, Zihan Lin, Zhen Cheng, You Zhou, Bo Xiong, Zhaofei Yu, Tiejun Huang
*   **🎯 研究目的**: 事件相机作为一种创新的生物启发式传感器，通过感知强度变化而非直接感知强度，并将这些变化记录为连续的“事件”流，与传统相机不同。从这些稀疏事件中重建强度一直是一个具有挑战性的问题。以往的方法主要集中于将运动诱导的事件转换为视频，或通过在事件相机采集端集成调制设备实现静态场景的强度成像。本研究旨在首次使用静态事件相机实现静态和动态场景下的荧光显微镜事件到强度转换。
*   **⭐ 主要发现**: 本文首次提出了一种名为“事件间隔显微镜”（Inter-event Interval Microscopy）的方法，实现了使用静态事件相机对静态和动态场景的荧光显微镜进行事件到强度转换。与主要依赖事件积分的传统方法不同，该方法利用事件之间的时间间隔信息，捕捉了更精细的强度变化。这一创新突破了事件相机在强度重建方面的局限性，使得事件相机能够应用于更广泛的显微成像领域，尤其是在高速动态场景下，有望为生物医学成像带来革命性的进步。

---

### [[EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection]](http://arxiv.org/abs/2504.04124v1)
<!-- 2025-04-05 -->
**📅 发布日期**: 2025-04-05

*   **👥 作者**: Muhammad Ahmed Ullah Khan, Abdul Hannan Khan, Andreas Dengel
*   **🎯 研究目的**: 事件相机具有更高的时间分辨率，并且与传统RGB相机相比，需要更少的存储和带宽。然而，由于事件基方法性能相对滞后，事件相机尚未在自动驾驶等性能关键型应用中取代传统相机。最近的事件基目标检测方法试图通过采用计算成本高昂的基于Transformer的解决方案来弥补这一差距。然而，由于其资源密集型组件，这些解决方案未能有效利用事件相机的稀疏性和更高的时间分辨率。此外，这些解决方案多是从视觉领域借鉴而来，缺乏对事件相机的特异性。本研究旨在探索更高效、更高性能的事件基实时交通目标检测替代方案。
*   **⭐ 主要发现**: 本文提出了Event Meta Formers (EMF)，一种用于事件基实时交通目标检测的高效且高性能的替代方案。EMF旨在克服现有基于Transformer的解决方案的资源密集型问题，并充分利用事件相机的稀疏性和高时间分辨率特性。通过针对事件数据特性进行优化，EMF能够更有效地处理事件流，实现更快的推理速度和更高的检测精度。这项工作为事件相机在自动驾驶等实时应用中的广泛部署铺平了道路，有望提升其在性能关键场景下的竞争力。

---

### [[Simultaneous Motion And Noise Estimation with Event Cameras]](http://arxiv.org/abs/2504.04029v1)
<!-- 2025-04-05 -->
**📅 发布日期**: 2025-04-05

*   **👥 作者**: Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
*   **🎯 研究目的**: 事件相机是新兴的视觉传感器，其噪声特性难以表征。现有的事件相机去噪方法通常将运动估计等其他任务分开考虑（即在去噪后顺序进行）。然而，运动是事件数据的内在组成部分，因为没有运动就无法感知场景边缘。本研究旨在提出一种同时估计运动（如自我运动、光流）和噪声的方法，以克服现有方法的局限性。
*   **⭐ 主要发现**: 本文提出了首个同时估计事件相机运动和噪声的方法。该方法具有灵活性，允许将广泛使用的对比度最大化框架中的一步运动估计替换为任何其他运动估计器，例如深度神经网络。实验结果表明，所提出的方法在E-MLB去噪基准测试中取得了最先进的结果。通过联合处理运动和噪声，该方法能够更准确地理解事件数据，从而在去噪和运动估计方面都取得了显著提升，为事件相机在复杂动态场景下的应用奠定了基础。

---

### [[EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling]](http://arxiv.org/abs/2504.02402v1)
<!-- 2025-04-03 -->
**📅 发布日期**: 2025-04-03

*   **👥 作者**: Hao Yin, Shi Guo, Xu Jia, Xudong XU, Lu Zhang, Si Liu, Dong Wang, Huchuan Lu, Tianfan Xue
*   **🎯 研究目的**: 当声波撞击物体时，会引起振动，从而产生高频且微小的视觉变化，这些变化可用于恢复声音。早期研究在采样率、带宽、视场和光学路径的简易性之间总是遇到权衡。事件相机硬件的最新进展显示出其在视觉声音恢复方面的巨大潜力，因为它在捕获高频信号方面具有卓越的能力。然而，现有基于事件的振动恢复方法对于声音恢复仍然不是最优的。本研究旨在提出一种新颖的非接触式声音恢复流程，充分利用事件流中的时空信息。
*   **⭐ 主要发现**: 本文提出了一种名为EvMic的新型非接触式声音恢复流程，该流程充分利用了事件流中的时空信息。首先，通过新颖的模拟流程生成了大型训练数据集。其次，设计了一个新的网络架构，能够有效建模事件数据的时空特性，从而从物体振动中恢复高频声音信号。实验结果表明，EvMic在非接触式声音恢复方面取得了显著的性能提升，克服了现有方法的局限性，为事件相机在音频视觉融合、远程监控等领域开辟了新的应用前景。

---

### [[Exploring Temporal Dynamics in Event-based Eye Tracker]](http://arxiv.org/abs/2503.23725v1)
<!-- 2025-03-31 -->
**📅 发布日期**: 2025-03-31

*   **👥 作者**: Hongwei Ren, Xiaopeng Lin, Hongxiang Huang, Yue Zhou, Bojun Cheng
*   **🎯 研究目的**: 眼动追踪是人机交互的关键技术，尤其是在AR、VR和XR等可穿戴设备中。使用基于帧的图像传感器实现高速高精度眼动追踪受到其有限时间分辨率的限制，这损害了对快速眼球动态（如扫视和眨眼）的准确捕获。事件相机受生物视觉系统启发，能够以极低的功耗和超高时间分辨率感知眼球运动，使其成为实现具有丰富时间动态的高速、高精度追踪的有前景的解决方案。本研究旨在提出一个有效的眼动追踪框架，通过彻底建模来自事件流的时间动态来捕获快速眼球运动。
*   **⭐ 主要发现**: 本文提出了TDTracker，一个有效的眼动追踪框架，该框架通过彻底建模来自事件流的时间动态来捕获快速眼球运动。TDTracker充分利用了事件相机超高时间分辨率的优势，能够精确捕捉扫视和眨眼等快速眼球动态，克服了传统帧相机在时间分辨率上的限制。该框架有望为可穿戴设备中的人机交互提供更准确、更实时的眼动追踪解决方案，从而提升用户体验和应用性能。

---

### [[SuperEIO: Self-Supervised Event Feature Learning for Event Inertial Odometry]](http://arxiv.org/abs/2503.22963v1)
<!-- 2025-03-29 -->
**📅 发布日期**: 2025-03-29

*   **👥 作者**: Peiyu Chen, Fuling Lin, Weipeng Guan, Peng Lu
*   **🎯 研究目的**: 事件相机异步输出低延迟事件流，有望在高速运动和挑战性光照条件下实现状态估计。与基于帧的相机不同，事件相机对运动的依赖性给鲁棒的事件特征检测和匹配带来了持续的挑战。近年来，基于学习的方法在特征检测和匹配方面表现出优于传统手工方法的鲁棒性，特别是在剧烈运动和HDR场景下。本研究旨在提出一个新颖的框架，利用基于学习的纯事件检测和IMU测量来实现事件惯性里程计。
*   **⭐ 主要发现**: 本文提出了SuperEIO，一个新颖的自监督事件特征学习框架，用于事件惯性里程计。该框架利用基于学习的纯事件检测和IMU测量，实现了在高速运动和挑战性光照条件下的鲁棒状态估计。SuperEIO的事件特征检测采用卷积神经网络在连续事件流下进行，通过自监督学习机制，克服了事件数据运动依赖性带来的挑战。实验结果表明，SuperEIO在事件惯性里程计任务中表现出卓越的鲁棒性和精度，为事件相机在自主导航和机器人领域的应用提供了强大的支持。

---

### [[Towards Mobile Sensing with Event Cameras on High-agility Resource-constrained Devices: A Survey]](http://arxiv.org/abs/2503.22943v2)
<!-- 2025-03-29 -->
**📅 发布日期**: 2025-03-29

*   **👥 作者**: Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen
*   **🎯 研究目的**: 随着移动设备应用复杂性的增加，这些设备正朝着高敏捷性方向发展。这种转变对移动传感提出了新的要求，特别是在实现高精度和低延迟方面。事件基视觉作为一种颠覆性范式，提供了高时间分辨率、低延迟和高能效，使其非常适合在高敏捷性平台上的高精度和低延迟传感任务。然而，存在大量噪声事件、缺乏固有的语义信息以及数据量大等问题，给资源受限的移动设备上的事件基数据处理带来了巨大挑战。本综述旨在对2014年至2024年期间的文献进行系统梳理，全面概述基于事件的移动传感系统。
*   **⭐ 主要发现**: 本综述全面概述了2014年至2024年期间基于事件的移动传感系统。它详细分析了事件相机在高敏捷性资源受限设备上进行移动传感的优势（如高时间分辨率、低延迟、能效）以及面临的挑战（如噪声事件、缺乏语义信息、数据量大）。综述涵盖了事件数据处理、算法优化、硬件集成和具体应用场景等多个方面。通过对现有研究的系统梳理，本综述为研究人员提供了事件相机在移动传感领域的研究现状、关键技术和未来发展方向的深入见解，有助于推动事件相机在智能手机、无人机等移动设备上的广泛应用。

---

### [[EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation]](http://arxiv.org/abs/2503.20268v1)
<!-- 2025-03-26 -->
**📅 发布日期**: 2025-03-26

*   **👥 作者**: Ziran Zhang, Xiaohui Li, Yihao Liu, Yujin Wang, Yueting Chen, Tianfan Xue, Shi Guo
*   **🎯 研究目的**: 在大运动场景下的视频帧插值（VFI）仍然具有挑战性，因为帧之间存在运动模糊。尽管事件相机可以捕获高时间分辨率的运动信息，但现有的基于事件的VFI方法在有限的训练数据和复杂的运动模式下表现不佳。本研究旨在提出一个新颖的框架，利用预训练的稳定视频扩散模型的强大先验知识以及事件相机精确的时间信息，以实现物理真实的大运动帧插值。
*   **⭐ 主要发现**: 本文引入了事件引导视频扩散模型（Event-Guided Video Diffusion Model, EGVD），这是一个新颖的框架，它利用预训练的稳定视频扩散模型的强大先验知识以及事件相机精确的时间信息，生成物理真实的中间帧。EGVD的核心是多模态运动条件生成器（Multi-modal Motion Condition Generator, MMCG），该模块有效整合RGB帧和事件信号来引导扩散过程。此外，研究采用了一种选择性微调策略，在保留预训练模型强大泛化能力的同时，使其适应事件数据。实验结果表明，EGVD在处理大运动场景下的视频帧插值方面取得了显著的性能提升，生成了更真实、更准确的中间帧。

---

### [[EventFly: Event Camera Perception from Ground to the Sky]](http://arxiv.org/abs/2503.19916v1)
<!-- 2025-03-25 -->
**📅 发布日期**: 2025-03-25

*   **👥 作者**: Lingdong Kong, Dongyue Lu, Xiang Xu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau
*   **🎯 研究目的**: 事件基密集感知中的跨平台适应性对于在车辆、无人机和四足机器人等不同设置中部署事件相机至关重要，因为每个平台都具有独特的运动动力学、视角和类别分布。本研究旨在提出一个框架，以实现事件相机感知的鲁棒跨平台适应性，从而克服不同平台带来的挑战。
*   **⭐ 主要发现**: 本文引入了EventFly框架，用于事件相机感知的鲁棒跨平台适应性。该方法包含三个关键组件：i) 事件激活先验（Event Activation Prior, EAP），用于识别目标域中的高激活区域，以最小化预测熵，从而促进自信的、领域自适应的预测；ii) EventBlend，一种数据混合策略，根据EAP驱动的相似性和密度图整合源域和目标域的事件体素网格，增强特征对齐；iii) EventMatch，一种双判别器技术，进一步对齐特征分布。实验结果表明，EventFly显著提高了事件相机在不同平台（如地面车辆到空中无人机）上的感知性能，展示了其在复杂多变环境中的强大适应性。

---
### [[A Survey on Event-driven 3D Reconstruction: Development under Different Categories]](http://arxiv.org/abs/2503.19753v3)
**📅 发布日期**: 2025-03-25

*   **👥 作者**: Chuanzhi Xu, Haoxian Zhou, Haodong Chen, Vera Chung, Qiang Qu
*   **🎯 研究目的**: 事件相机凭借其高时间分辨率、低延迟和高动态范围等独特优势，在三维重建领域日益受到关注，尤其适用于快速运动和复杂光照条件下的精确重建。本研究旨在对事件驱动的三维重建方法进行全面综述，系统梳理该领域的发展脉络、创新点和最新进展，以期为未来的研究提供坚实的基础和方向指引。
*   **⭐ 主要发现**: 本综述的核心贡献在于对事件驱动三维重建领域进行了全面而系统的梳理。论文首先将现有方法划分为立体、单目和多模态系统，并进一步根据几何、基于学习和混合方法对近期发展进行了深入分类。特别地，本研究还涵盖了该领域的前沿新兴趋势，如结合事件数据的神经辐射场（NeRF）和3D高斯泼溅技术。通过按时间顺序结构化相关工作，本综述清晰地展现了该领域的创新演进和技术突破，为研究人员理解当前进展、识别未来研究方向提供了宝贵的参考和支持。

---
### [[EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation]](http://arxiv.org/abs/2503.18552v2)
**📅 发布日期**: 2025-03-24

*   **👥 作者**: Qiang Qu, Ming Li, Xiaoming Chen, Tongliang Liu
*   **🎯 研究目的**: 传统条件人体动画通过从视频数据中提取姿态线索来驱动静态参考图像的动画生成。然而，这些基于视频的线索常面临时间分辨率低、运动模糊以及在挑战性光照条件下性能不可靠等问题。与此形成对比的是，事件相机能够天然地提供鲁棒且高时间分辨率的运动信息，对运动模糊、低光环境和曝光变化具有更强的适应性。基于此，本研究旨在提出EvAnimate，作为首个利用事件流作为鲁棒、精确运动线索的条件人体图像动画方法。
*   **⭐ 主要发现**: 本文提出了EvAnimate，这是首个利用事件流作为条件人体图像动画的鲁棒且精确运动线索的方法。该方法与当前流行的扩散模型完全兼容，其实现得益于将异步事件数据编码成一种专门的三通道表示。通过利用事件相机固有的高时间分辨率和对恶劣环境的鲁棒性，EvAnimate有望克服传统视频线索的局限性，显著提升人体动画的质量和稳定性。

---
### [[PS-EIP: Robust Photometric Stereo Based on Event Interval Profile]](http://arxiv.org/abs/2503.18341v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-24

*   **👥 作者**: Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, Tsuyoshi Takatani
*   **🎯 研究目的**:
    现有的基于事件相机的光度立体方法（EventPS）能够从移动定向光源下对数朗伯反射变化触发的事件中恢复物体表面法线，具有能效优势。然而，EventPS独立处理每个事件间隔，导致其对噪声、阴影和非朗伯反射等异常情况非常敏感，鲁棒性不足。本研究旨在提出一种更鲁棒的光度立体方法，以克服EventPS的这些局限性，从而能够从事件间隔的时间序列剖面中准确且稳定地恢复像素级的表面法线。

*   **⭐ 主要发现**:
    本文提出了一种名为基于事件间隔剖面的光度立体方法（PS-EIP）。该方法通过利用事件间隔剖面的时间序列连续性，并引入基于剖面形状的异常值检测机制，显著增强了对阴影和镜面反射等异常值的鲁棒性。与现有方法独立处理事件间隔不同，PS-EIP考虑了事件间隔的整体时间序列特征。通过在真实3D打印物体的事件数据上进行的实验，验证了PS-EIP在复杂光照和反射条件下恢复像素级表面法线的有效性和优越的鲁棒性。这项工作为基于事件相机的光度立体技术提供了一种更可靠、更实用的解决方案，拓宽了其在真实世界应用中的潜力。

---
### [[基于事件相机的光流与图像强度无监督联合学习]](http://arxiv.org/abs/2503.17262v1)
**📅 发布日期**: 2025-03-21

*   **👥 作者**: Shuang Guo, Friedhelm Hamann, Guillermo Gallego
*   **🎯 研究目的**: 事件相机通过感知场景中的运动来获取外观信息，这意味着运动和外观是紧密耦合、同生共存的，并编码在输出的事件流中。然而，以往的研究通常将光流（运动）和图像强度（外观）的恢复视为独立的任务，这与事件相机的本质不符，并且忽略了这两个视觉量之间固有的关联性。本文旨在解决这一问题，提出一个无监督学习框架，利用单个网络同时估计光流和图像强度，从而更好地利用事件相机输出数据的特性，克服现有方法的局限性。
*   **⭐ 主要发现**:
    *   **创新性框架：** 提出了一种新颖的无监督学习框架，能够通过单个神经网络同时估计事件相机的光流（运动信息）和图像强度（外观信息），克服了以往将两者视为独立任务的局限性。
    *   **理论推导：** 从事件生成模型出发，首次推导出了基于事件的光度误差（event-based photometric error）。这一创新性的误差项能够更好地捕捉事件数据中运动与外观的内在联系。
    *   **损失函数构建：** 将新推导的光度误差与对比度最大化框架相结合，构建了一个全面的损失函数。这种综合性的损失函数使得模型能够更有效地利用事件数据进行联合优化。
    *   **潜在影响：** 通过无监督的联合学习，该方法更符合事件相机数据生成的本质，有望在复杂动态场景下实现更鲁棒、更精确的光流和图像强度估计，为事件相机在机器人视觉、自动驾驶等领域的应用提供新的解决方案。

---
### [[Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition]](http://arxiv.org/abs/2503.17132v3)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-21

*   **👥 作者**: Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot
*   **🎯 研究目的**: 本文旨在探索脉冲神经网络（SNNs）与事件相机在保护隐私的人体行为识别（HAR）中的应用潜力。事件相机捕捉运动轮廓的独特能力，结合SNNs通过脉冲处理时空数据的专长，为基于事件的HAR提供了高度协同的兼容性。然而，以往的研究受限于SNNs处理长期时间信息的能力，而这对于精确的人体行为识别至关重要。因此，本研究的核心目标是引入新颖的框架来解决SNNs在处理长期时间信息方面的局限性，从而提升基于事件的HAR的性能。
*   **⭐ 主要发现**: 为了克服SNNs在处理长期时间信息方面的不足，论文提出了两个新颖的框架：基于时间段的SNN（Temporal Segment-based SNN, TS-SNN）和3D卷积SNN（3D Convolutional SNN, 3D-SNN）。其中，TS-SNN通过将动作划分为更短的片段来有效地提取长期时间信息。虽然摘要被截断，但从其设计目的和名称来看，3D-SNN很可能通过利用3D卷积的优势，直接从事件数据中捕获丰富的时空特征，进一步增强SNN处理复杂和长期行为的能力。这些创新有望显著提升基于事件的人体行为识别的准确性和鲁棒性，同时充分利用事件相机在隐私保护方面的固有优势以及SNN在处理稀疏时空数据方面的效率。

---
### [[Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft]](http://arxiv.org/abs/2503.12732v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-17

*   **👥 作者**: Zibin Liu, Banglei Guan, Yang Shang, Yifei Bian, Pengju Sun, Qifeng Yu
*   **🎯 研究目的**: 非合作航天器的姿态跟踪是空间探索和在轨服务中的关键技术，但目前仍是一个尚未完全解决的难题。传统的相机在面对运动模糊和极端光照等复杂环境时，往往难以有效工作。事件相机凭借其高动态范围、高时间分辨率和低功耗等优势，有望克服这些挑战。本研究旨在利用立体事件相机，提出一种基于线特征的姿态跟踪方法，以解决标准在轨观测任务中非合作航天器的精确姿态估计问题。
*   **⭐ 主要发现**: 本文提出了一种创新的、基于线特征的非合作航天器姿态跟踪方法，该方法充分利用了立体事件相机的独特优势。其核心贡献包括：
    *   **线框模型估计与重建**: 论文首先利用立体事件流的时空一致性，实现了非合作航天器线框模型的精确估计和基于线特征的三维重建。这一创新步骤为后续的姿态跟踪奠定了坚实的基础。
    *   **克服传统相机局限**: 通过利用事件相机固有的高动态范围和高时间分辨率特性，该方法有望有效解决传统相机在极端光照和快速运动下产生的运动模糊等常见问题，显著提升了在轨观测任务的鲁棒性和适应性。
    *   **高效的姿态跟踪策略**: (根据摘要推断) 在线框模型重建的基础上，论文进一步开发了一种高效的姿态跟踪策略，以实现对非合作航天器6自由度姿态的实时、精确跟踪，为未来的空间任务提供了新的解决方案。

---
### [[EgoEvGesture: 基于事件相机的第一人称手势识别]](http://arxiv.org/abs/2503.12419v2)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-16

*   **👥 作者**: Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang, Jian Bai
*   **🎯 研究目的**: 第一人称手势识别是增强自然人机交互的关键技术。然而，传统的基于RGB的解决方案在动态场景中存在运动模糊和光照变化等问题。尽管事件相机在处理高动态范围和超低功耗方面显示出独特优势，但现有的基于RGB的架构由于其同步帧的特性，在处理异步事件流时面临固有限制。此外，从第一人称视角来看，事件相机记录的数据包含了头部运动和手势产生的事件，这增加了手势识别的复杂性。本研究旨在解决这些挑战，提出一种专门为事件数据处理设计的新型网络架构，以期在第一人称手势识别中充分利用事件相机的优势。
*   **⭐ 主要发现**: 为了解决传统RGB方案的局限性以及事件相机数据处理的复杂性（特别是第一人称视角下头部运动与手势事件的混淆），本论文提出了一种新颖的网络架构，该架构专门为事件数据处理而设计。该架构融入了（1）一个轻量级...（摘要在此处被截断，但可推断其包含针对事件数据特性优化的创新组件）。这项工作为利用事件相机进行鲁棒的第一人称手势识别提供了新的思路，有望显著提升动态场景下人机交互的自然性和效率。

---
### [[EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation]](http://arxiv.org/abs/2503.11371v2)
**📅 发布日期**: 2025-03-14

*   **👥 作者**: Zengyu Wan, Wei Zhai, Yang Cao, Zhengjun Zha
*   **🎯 研究目的**: 视觉3D运动估计旨在基于视觉线索推断2D像素在3D空间中的运动。然而，其核心挑战在于深度变化引起的时空运动不一致性，这破坏了以往运动估计框架中局部空间或时间运动平滑性的假设。为了应对这一挑战，本文旨在提出一个新颖的基于事件相机（Event Camera）的框架EMoTive。事件相机通过对场景变化的连续自适应像素级响应，为3D运动估计提供了新的可能性。本研究的核心目标是利用事件数据，通过建模时空轨迹来有效表征局部异构的时空运动，从而实现更准确、鲁棒的3D运动估计。
*   **⭐ 主要发现**: 本文提出了EMoTive，一个创新的基于事件的框架，用于3D运动估计。其主要贡献和创新点在于：
    *   **事件引导的轨迹建模**: EMoTive通过事件引导的非均匀参数曲线来建模时空轨迹，这使得它能够有效地表征局部异构的时空运动，从而克服了传统方法在处理由深度变化引起的不一致运动时的局限性。
    *   **克服传统假设**: 与依赖局部空间或时间运动平滑性假设的现有框架不同，EMoTive能够适应并捕捉场景中复杂的、非平滑的运动模式。
    *   **引入Event Kymograph**: 论文特别引入了“事件示波图”（Event Kymograph）这一事件投影方法，它利用连续的时间投影，进一步增强了对运动的捕捉能力和细节表征。
    *   **潜在影响**: EMoTive为在复杂动态场景中实现精确的3D运动估计提供了一种新颖且有效的解决方案，特别是在传统视觉方法受限于运动不一致性的情况下。这有望推动自动驾驶、机器人视觉和虚拟现实等领域在运动感知方面的进步。

---
### [[基于神经形态摄像机的非视距光无线通信]](http://arxiv.org/abs/2503.11226v1)
**📅 发布日期**: 2025-03-14

*   **👥 作者**: Abbaas Alif Mohamed Nishar, Alireza Marefat, Ashwin Ashok
*   **🎯 研究目的**: 本文旨在探索利用神经形态（事件）摄像机进行被动式光无线通信（OWC）。研究背景是神经形态摄像机受生物视觉系统启发，能以高时间分辨率和效率捕获光照变化，产生事件流而非传统图像。核心目标是利用其异步检测光照变化的能力来解码通过物体反射光线传输的数据，并提出一种新颖的系统，将此概念扩展到通过日常物体被动反射实现非视距（NLoS）可见光通信（VLC）场景。这为解决传统视距通信的局限性，特别是在非视距环境下实现高效、被动的数据传输提供了新途径。
*   **⭐ 主要发现**: 本文的核心贡献在于提出并验证了一种利用神经形态摄像机实现被动式光无线通信（OWC）的新颖系统。其主要创新点是将这一概念首次扩展到非视距（NLoS）场景，通过利用日常物体的被动光反射来解码数据。实验结果有力地证明了使用神经形态摄像机进行可见光通信（VLC）的可行性及其显著优势，并对系统性能进行了详细的表征。这一研究为在复杂、非视距环境中实现高效、低功耗的光无线通信开辟了新的途径，对未来通信领域具有潜在影响。

---
### [ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and Spiking Neural Network](http://arxiv.org/abs/2503.09985v2)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-13

*   **👥 作者**: Qiang Zhang, Jiahang Cao, Jingkai Sun, Yecheng Shao, Gang Han, Wen Zhao, Yijie Guo, Renjing Xu
*   **🎯 研究目的**: 近年来，四足机器人技术在感知和运动控制方面取得了显著进展，尤其是在利用强化学习实现复杂环境下的高难度动作方面。然而，现有的视觉传感器（如深度相机）存在局限性：其操作频率相对于关节控制较低，且对光照敏感，这阻碍了机器人在室外环境的部署。此外，传感器和控制系统中深度神经网络（DNNs）的使用也增加了计算需求。为解决这些问题，本文旨在引入受生物启发的事件相机和脉冲神经网络（SNNs），以实现四足机器人更高级的跑酷任务。研究目的在于克服传统视觉传感器和DNN的局限，提升机器人在复杂动态环境中的感知效率和运动控制鲁棒性。
*   **⭐ 主要发现**: 本文的核心贡献在于创新性地将受生物启发的事件相机与脉冲神经网络（SNNs）相结合，应用于高难度的四足机器人跑酷任务。事件相机能够高效捕捉动态视觉数据，而SNNs则以其独特的脉冲序列处理方式，高效地模拟生物感知过程，从而有效解决了传统视觉传感器在频率和光照敏感性方面的不足，并降低了深度神经网络带来的计算负担。尽管摘要中实验结果部分被截断，但研究明确指出，实验结果证明了该方法在实现复杂四足跑酷任务方面的有效性。这一研究为未来机器人感知与控制系统提供了新的范式，有望推动机器人技术在动态、复杂环境中的鲁棒性和效率达到新高度，尤其是在计算资源受限或需要快速响应的场景中。

---
### [[A PyTorch-Enabled Tool for Synthetic Event Camera Data Generation and Algorithm Development]](http://arxiv.org/abs/2503.09754v1)
<!-- 2025-03-12 -->
**📅 发布日期**: 2025-03-12

*   **👥 作者**: Joseph L. Greene, Adrish Kar, Ignacio Galindo, Elijah Quiles, Elliott Chen, Matthew Anderson
*   **🎯 研究目的**: 事件相机（或称神经形态相机）通过异步报告亮度变化来编码场景，具有高动态范围、高时间分辨率和低数据带宽等优势。然而，其在特定研究领域的应用受到诸多限制，包括商业可用性不足、现有数据集匮乏，以及难以预测其非线性光学编码、独特噪声模型和基于张量的数据处理需求带来的影响。本研究旨在解决这些障碍，通过提供一个强大的工具来促进事件相机技术在更广泛研究任务中的应用和发展。
*   **⭐ 主要发现**: 为应对事件相机数据和模型预测的挑战，本研究的核心贡献是推出了基于PyTorch的Python库——Synthetic Events for Neural Processing and Integration (SENPI)。SENPI是一个用于模拟和处理事件相机数据的工具，其核心创新点在于包含了一个可微分的数字孪生。这一工具的引入，有望显著降低事件相机研究的门槛，为研究人员提供生成合成数据和开发相关算法的强大平台，从而加速事件相机技术在各个领域的应用和发展。

---
### [Ev-Layout: A Large-scale Event-based Multi-modal Dataset for Indoor Layout Estimation and Tracking](http://arxiv.org/abs/2503.08370v1)
**📅 发布日期**: 2025-03-11

*   **👥 作者**: Xucheng Guo, Yiran Shen, Xiaofang Xiao, Yuanfeng Zhou, Lin Wang
*   **🎯 研究目的**: 该研究旨在推出Ev-Layout，一个新颖的大规模事件基多模态数据集，其核心目标是为室内布局估计和跟踪任务提供一个全面的数据资源。当前，在处理动态室内环境下的布局估计时，现有数据集可能在结合多种传感器数据和分析环境因素影响方面存在不足。Ev-Layout旨在通过整合RGB图像、仿生事件数据、惯性测量单元（IMU）以及环境光照条件，解决在运动中进行室内布局估计的挑战，并特别关注运动速度和光照条件对估计准确性的潜在影响，从而推动该领域的研究进展。
*   **⭐ 主要发现**: 论文的核心贡献在于发布了Ev-Layout数据集，这是一个专为室内布局估计和跟踪设计的新型、大规模、事件基多模态数据集。
    *   **创新性数据采集**: Ev-Layout采用独特的混合数据采集平台（结合头戴式显示器和VR接口），创新性地整合了RGB相机和仿生事件相机，以捕捉运动中的室内布局，从而反映真实世界的动态场景。
    *   **丰富的数据内容**: 除了传统的RGB图像（超过77.13万张，其中3.9万张已标注室内布局）和海量事件数据（超过100亿个事件数据点），Ev-Layout还包含了惯性测量单元（IMU）的时间序列数据和环境光照条件记录。
    *   **独特研究价值**: 这种多模态数据的整合使得研究人员能够深入分析运动速度和光照条件对布局估计准确性的潜在影响，填补了现有数据集在考虑这些环境因素方面的空白。
    *   **大规模数据集**: 数据集规模庞大，包含2.5K个序列，为训练和评估鲁棒的室内布局估计和跟踪算法提供了充足的数据支持。
    *   **潜在影响**: Ev-Layout的发布为计算机视觉社区提供了一个宝贵的资源，将极大地推动事件基视觉在动态室内环境理解、鲁棒布局估计和跟踪算法开发等方面的研究进展，特别是在考虑复杂运动和多变光照条件下的应用。

---
### [[Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables]](http://arxiv.org/abs/2503.07825v1)
**📅 发布日期**: 2025-03-10

*   **👥 作者**: Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, Louis Berridge
*   **🎯 研究目的**: 本文旨在推动可穿戴技术的发展，提出一种针对智能眼镜优化的、实时、超低功耗的事件相机系统，以实现自然手势控制，从而显著改善用户体验。当前计算机视觉中的手势识别虽有显著进步，但在创建直观、能适应不同用户和环境、且能效足够支持实际可穿戴应用方面仍面临关键挑战。本研究的核心目标是解决这些挑战，开发一个实用、高效且用户友好的手势识别系统。
*   **⭐ 主要发现**: 本文提出了“Helios 2.0”系统，这是一个针对事件传感器可穿戴设备优化的鲁棒、超低功耗手势识别系统。该系统通过精心挑选的“微手势”来应对现有挑战，这些微手势包括食指上的横向拇指滑动（双向）以及拇指与食指尖之间的双捏。这些以人为中心的设计利用了自然的肢体动作，确保了直观的可用性，无需用户学习复杂的指令。Helios 2.0系统通过提供一种高效、节能且自然的交互方式，为智能眼镜带来了突破性的用户体验提升，为可穿戴设备领域的实际应用奠定了基础。
### [[Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic Scene Optical Flow]](http://arxiv.org/abs/2503.06992v2)
**📅 发布日期**: 2025-03-10

*   **👥 作者**: Hanyu Zhou, Haonan Wang, Haoyue Liu, Yuxing Duan, Yi Chang, Luxin Yan
*   **🎯 研究目的**: 高动态场景下的光流估计是一项极具挑战性的任务。传统的基于帧的成像方法由于大位移运动会导致空间模糊和时间上的不连续性，从而损害光流的时空特征。尽管现有方法尝试引入事件相机来直接融合两种模态的时空特征，但由于帧和事件数据表示之间的巨大异构性（模态鸿沟），这种直接融合的效果并不理想。本文旨在解决这一模态鸿沟问题，探索一种有效的中间桥梁，以实现高动态场景下帧与事件模态之间鲁棒且高效的时空融合，从而提升光流估计的准确性。
*   **⭐ 主要发现**: 本文提出了一种新颖的帧与事件模态之间的通用时空融合方法，用于高动态场景下的光流估计。其核心创新在于探索并利用一个“通用潜在空间”（common-latent space）作为中间桥梁，以有效缓解帧和事件模态之间由于异构数据表示而产生的巨大鸿沟。这种方法克服了现有直接融合方案的局限性，实现了更鲁棒、更精确的时空特征融合。论文还提到了包括视觉边界定位在内的具体实现细节。通过这种通用潜在空间的桥接，该方法有望显著提升高动态场景下光流估计的性能，为处理复杂运动场景提供新的解决方案。
### [[LLaFEA: Frame-Event Complementary Fusion for Fine-Grained Spatiotemporal Understanding in LMMs]](http://arxiv.org/abs/2503.06934v1)
**📅 发布日期**: 2025-03-10

*   **👥 作者**: Hanyu Zhou, Gim Hee Lee
*   **🎯 研究目的**: 本文旨在解决大型多模态模型（LMMs）在细粒度时空推理方面的不足。尽管LMMs在场景理解上表现出色，但由于语言和视觉表征之间的对齐较弱，它们难以进行精确的时空理解。现有方法依赖于从基于帧的视频中编码的视觉空间来映射文本位置和持续时间，但这种方法存在时间稀疏性问题，严重限制了语言与视觉之间的时间协调能力。因此，本研究的核心目标是引入一种新的框架LLaFEA，通过利用事件相机实现时间密集的感知和帧-事件融合，从而显著提升LMMs的细粒度时空理解能力。
*   **⭐ 主要发现**: 本文的核心贡献是提出了LLaFEA（Large Language and Frame-Event Assistant）框架，旨在解决现有大型多模态模型（LMMs）在细粒度时空理解中面临的时间稀疏性问题。LLaFEA的创新之处在于其首次利用事件相机进行时间密集的感知，并实现了帧数据与事件数据的互补融合。
    具体而言，LLaFEA采用了一种新颖的跨注意力机制，以有效整合来自帧数据的互补空间特征和来自事件数据的密集时间特征，从而克服了传统帧视频的稀疏性限制。在此基础上，模型进一步通过自注意力匹配机制，实现了全局时空关联，显著提升了对复杂动态场景的理解能力。此外，LLaFEA还将文本中的位置和持续时间标记嵌入到融合后的视觉表示中，进一步增强了语言与视觉之间的精确对齐。这些创新使得LLaFEA能够实现对视频内容更精细、更准确的时空推理。

---
### [Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms](http://arxiv.org/abs/2503.06484v1)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-09

*   **👥 作者**: Xiao Wang, Yuehang Li, Fuling Wang, Bo Jiang, Yaowei Wang, Yonghong Tian, Jin Tang, Bin Luo
*   **🎯 研究目的**: 准确的手语理解对于残障人士的交流至关重要。当前的手语翻译算法主要依赖于RGB帧，但这种方法存在固定帧率、光照条件变化以及快速手部运动引起的运动模糊等局限性。受事件相机在其他领域成功应用的启发，本研究旨在利用事件流辅助RGB相机捕获手势数据，以克服现有方法的挑战，从而提升手语翻译的准确性和鲁棒性。
*   **⭐ 主要发现**: 本文的核心贡献在于提出了利用事件流辅助RGB帧进行手语翻译的新范式。具体来说，研究团队首先利用DVS346相机收集并构建了一个大规模的RGB-事件手语翻译基准数据集，命名为VECSL。该数据集包含15,676个RGB-事件样本、15,191个词汇（glosses），覆盖了2,568个汉字，且样本采集多样化，为手语翻译研究提供了宝贵的资源。通过结合事件流的高时间分辨率和对光照变化的鲁棒性，本研究旨在克服传统RGB帧在固定帧率、光照不均和运动模糊等方面的限制，从而显著提升手语理解和翻译的准确性与鲁棒性。尽管摘要未详细展开，但论文标题明确指出，研究还提出了相应的算法来有效利用帧和事件流数据进行手语翻译，这有望为手语识别和翻译领域带来新的突破和研究方向。

---
### [[SDTrack: A Baseline for Event-based Tracking via Spiking Neural Networks]](http://arxiv.org/abs/2503.08703v2)
**📅 发布日期**: 2025-03-09

*   **👥 作者**: Yimeng Shan, Zhenbang Ren, Haodi Wu, Wenjie Wei, Rui-Jie Zhu, Shuai Wang, Dehao Zhang, Yichen Xiao, Jieyuan Zhang, Kexin Shi, Jingzhinan Wang, Jason K. Eshraghian, Haicheng Qu, Jiqing Zhang, Malu Zhang, Yang Yang
*   **🎯 研究目的**: 事件相机凭借其卓越的时间分辨率、动态范围、能效和像素带宽，在动态场景追踪中具有巨大潜力。脉冲神经网络（SNNs）因其离散的脉冲信号特性，与事件数据天然契合，是事件基追踪的理想选择。然而，当前结合人工神经网络（ANNs）和SNNs的方法，以及次优的架构，往往牺牲了能源效率并限制了追踪性能。本研究旨在解决这些局限性，开发一种更高效、性能更优越的纯脉冲驱动的事件基追踪方法。
*   **⭐ 主要发现**: 本研究提出了SDTrack，这是首个基于Transformer的纯脉冲驱动事件基追踪流水线，旨在克服现有方法的能效和性能瓶颈。核心贡献包括：
    1.  **首个Transformer-based脉冲驱动追踪流水线**: SDTrack首次将Transformer架构引入到SNN驱动的事件基追踪任务中，为该领域提供了一个高性能且能效优化的新基线。
    2.  **全局轨迹提示（GTP）方法**: 提出了一种创新的全局轨迹提示（GTP）方法。GTP能够有效捕获全局轨迹信息，并将其与事件流聚合到事件图像中，从而显著增强了时空表示能力，为后续的追踪提供了更丰富、更准确的特征。
    3.  **SDTrack追踪器**: 基于上述创新，研究团队开发了完整的SDTrack追踪器，该追踪器充分利用了事件相机数据和SNNs的优势，有望在事件基追踪领域实现更高的性能和能效。

---
### [[Combined Physics and Event Camera Simulator for Slip Detection]](http://arxiv.org/abs/2503.04838v2)
<!-- 论文发布日期，格式：YYYY-MM-DD -->
**📅 发布日期**: 2025-03-05

*   **👥 作者**: Thilo Reinold, Suman Ghosh, Guillermo Gallego
*   **🎯 研究目的**: 机器人操作在工业制造等领域中是一项常见且关键的任务。其中，检测物体何时从机器人手中滑脱对于确保操作的安全性和可靠性至关重要。事件相机因其独特的像素级亮度变化检测能力和高时间分辨率，在安装于机器人末端执行器时展现出优雅的特性：由于它们仅检测相对于自身视角的运动，因此被牢固抓取的物体不会产生事件，而一旦物体开始滑脱，事件相机便会立即触发事件。为了深入研究这一特性，并支持分析方法和机器学习模型的训练，获取具有代表性的数据集变得不可或缺。然而，目前大多数基于事件数据的滑脱检测研究都依赖于真实世界场景和手动数据收集，以及额外的设置，这限制了研究的进展。本文旨在解决这一数据获取的挑战，为滑脱检测研究提供一个有效的解决方案。
*   **⭐ 主要发现**: 本文的核心贡献在于提出了一个**结合了物理引擎和事件相机模型的模拟器**，专门用于生成机器人抓取物体滑脱的事件数据。该模拟器能够克服当前研究中依赖真实世界场景和手动数据收集的局限性，为滑脱检测研究提供了一个可控且可扩展的数据生成平台。通过模拟物理交互和事件相机的响应机制，该模拟器能够生成具有代表性的合成数据集，这些数据对于：
    *   **支持分析方法研究**：提供精确控制的实验条件，以便深入理解滑脱现象和事件相机响应。
    *   **训练机器学习模型**：为基于事件数据的滑脱检测算法提供大规模、多样化的训练数据，从而提高模型的鲁棒性和泛化能力。
    *   **加速领域发展**：降低了获取高质量滑脱事件数据的门槛，有望加速事件相机在机器人抓取和操作领域的研究与应用。
    这一创新性的模拟器为事件相机在机器人末端执行器上的滑脱检测应用奠定了坚实的基础，是推动该领域发展的重要一步。

---
### [[Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers]](http://arxiv.org/abs/2503.03307v2)
**📅 发布日期**: 2025-03-05

*   **👥 作者**: Ji Zhao, Banglei Guan, Zibin Liu, Laurent Kneip
*   **🎯 研究目的**: 当前用于事件相机自我运动估计的稀疏几何求解器通常假设旋转位移已知（例如由IMU提供），因此只能恢复平移运动参数。然而，使用稀疏几何求解器恢复全自由度（Full-DoF）运动参数（即同时包括旋转和平移）是一个更具挑战性且尚未被充分研究的问题。本研究旨在解决这一空白，提出一种统一的框架来同时估计事件相机的旋转和平移速度，从而实现全自由度自我运动估计。
*   **⭐ 主要发现**: 论文的核心贡献是提出了一系列创新的几何求解器，能够在统一框架下同时估计事件相机的旋转和平移速度。这些求解器利用了由线段诱导的事件流形（event manifolds）。其问题表述基于两种主要关系：一是线的入射关系（incidence relation for lines），二是作者提出的一种新颖的法向量共面关系（novel coplanarity relation for normal vectors）。研究结果首次证明了使用稀疏几何求解器，在不依赖外部旋转信息（如IMU）的情况下，恢复事件相机全自由度自我运动参数（包括角速度和线速度）的可能性。

---
### [[BAT: Learning Event-based Optical Flow with Bidirectional Adaptive Temporal Correlation]](http://arxiv.org/abs/2503.03256v1)
**📅 发布日期**: 2025-03-05

*   **👥 作者**: Gangwei Xu, Haotong Lin, Zhaoxing Zhang, Hongcheng Luo, Haiyang Sun, Xin Yang
*   **🎯 研究目的**: 事件相机以其高动态范围和高时间分辨率的视觉信息而闻名，这使其在复杂光照条件和快速移动物体的场景中，对光流估计具有显著优势。然而，当前先进的事件相机光流方法大多沿用已有的基于图像的框架，这导致事件数据的空间稀疏性限制了它们的性能。本文旨在提出BAT（Bidirectional Adaptive Temporal correlation），一个创新的框架，通过利用双向自适应时间相关性来估计基于事件的光流，以克服现有方法的局限性，实现准确且空间密集的事件光流估计。
*   **⭐ 主要发现**: 本文的核心贡献是提出了BAT框架，一个用于事件相机光流估计的创新方法，该方法巧妙地利用了双向自适应时间相关性。BAT框架包含三项新颖设计。其中，第一项是“双向时间相关性”，它能够有效地将双向时间密集型的运动线索转换为空间密集型线索，从而实现准确且空间密集的事件光流估计。通过这些创新，BAT旨在显著提升事件相机在处理空间稀疏数据时的光流估计性能和准确性，为复杂场景下的视觉感知提供更可靠的解决方案。

---

---

## 生成说明
- 本报告由AI模型自动生成，摘要内容仅供参考。
- 如有错误或遗漏，请以原始论文为准。
