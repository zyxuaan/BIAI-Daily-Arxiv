"""
è®ºæ–‡æ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆè®ºæ–‡æ‘˜è¦
"""
import os
import re
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
from datetime import datetime
import pytz
from config.settings import LLM_CONFIG

class ModelClient:
    """è¯­è¨€æ¨¡å‹APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or LLM_CONFIG['model']
        self.api_url = f"{LLM_CONFIG['api_url']}/{self.model}:generateContent"
        self.timeout = LLM_CONFIG.get('timeout', 60) # å¢åŠ è¶…æ—¶æ—¶é—´
        
    def _create_headers(self) -> Dict[str, str]:
        """åˆ›å»ºè¯·æ±‚å¤´"""
        return {
            "Content-Type": "application/json"
        }
    
    def _create_request_body(
        self, 
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºè¯·æ±‚ä½“"""
        prompt = messages[-1]["content"]
        
        return {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature or LLM_CONFIG['temperature'],
                "maxOutputTokens": max_tokens or LLM_CONFIG['max_output_tokens'],
                "topP": LLM_CONFIG['top_p'],
                "topK": LLM_CONFIG['top_k']
            }
        }
    
    def _extract_content_from_response(self, result: Dict[str, Any]) -> str:
        """ä»APIå“åº”ä¸­æå–å†…å®¹ï¼Œå¤„ç†ä¸åŒçš„å“åº”æ ¼å¼"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "error" in result:
                error_msg = result["error"].get("message", "Unknown API error")
                raise ValueError(f"APIé”™è¯¯: {error_msg}")
            
            # æ£€æŸ¥candidates
            if not result.get("candidates"):
                raise ValueError("APIå“åº”ä¸­æ²¡æœ‰candidateså­—æ®µ")
            
            candidates = result["candidates"]
            if not candidates:
                raise ValueError("APIå“åº”ä¸­candidatesä¸ºç©º")
            
            # è·å–ç¬¬ä¸€ä¸ªå€™é€‰é¡¹
            candidate = candidates[0]
            
            # æ£€æŸ¥æ˜¯å¦è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢
            if candidate.get("finishReason") == "SAFETY":
                raise ValueError("å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢")
            
            # å°è¯•ä¸åŒçš„å†…å®¹æå–æ–¹å¼
            content = None
            
            # æ–¹å¼1: æ ‡å‡†æ ¼å¼ candidates[0].content.parts[0].text
            if "content" in candidate:
                content_obj = candidate["content"]
                if isinstance(content_obj, dict):
                    if "parts" in content_obj and content_obj["parts"]:
                        parts = content_obj["parts"]
                        if isinstance(parts, list) and len(parts) > 0:
                            first_part = parts[0]
                            if isinstance(first_part, dict) and "text" in first_part:
                                content = first_part["text"]
                    elif "text" in content_obj:
                        # æ–¹å¼2: ç›´æ¥åœ¨contentä¸­æœ‰textå­—æ®µ
                        content = content_obj["text"]
                elif isinstance(content_obj, str):
                    # æ–¹å¼3: contentç›´æ¥æ˜¯å­—ç¬¦ä¸²
                    content = content_obj
            
            # æ–¹å¼4: ç›´æ¥åœ¨candidateä¸­æŸ¥æ‰¾text
            if not content and "text" in candidate:
                content = candidate["text"]
            
            # æ–¹å¼5: æŸ¥æ‰¾messageå­—æ®µ
            if not content and "message" in candidate:
                message = candidate["message"]
                if isinstance(message, dict) and "content" in message:
                    content = message["content"]
                elif isinstance(message, str):
                    content = message
            
            if not content:
                # æ‰“å°å“åº”ç»“æ„ä»¥ä¾¿è°ƒè¯•
                print(f"è°ƒè¯•ä¿¡æ¯ - APIå“åº”ç»“æ„: {json.dumps(result, indent=2, ensure_ascii=False)}")
                raise ValueError("æ— æ³•ä»APIå“åº”ä¸­æå–å†…å®¹")
            
            if not isinstance(content, str):
                raise ValueError(f"æå–çš„å†…å®¹ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(content)}")
            
            return content.strip()
            
        except Exception as e:
            print(f"è§£æAPIå“åº”æ—¶å‡ºé”™: {e}")
            print(f"å“åº”ç»“æ„: {json.dumps(result, indent=2, ensure_ascii=False)}")
            raise

    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºèŠå¤©å®Œæˆ"""
        headers = self._create_headers()
        data = self._create_request_body(messages, temperature, max_tokens)
        
        last_exception = None
        
        for attempt in range(LLM_CONFIG['retry_count']):
            try:
                print(f"å°è¯•APIè°ƒç”¨ (ç¬¬{attempt + 1}æ¬¡)...")
                response = requests.post(
                    f"{self.api_url}?key={self.api_key}",
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç 
                if response.status_code != 200:
                    error_msg = f"HTTPé”™è¯¯ {response.status_code}: {response.text}"
                    print(f"HTTPé”™è¯¯: {error_msg}")
                    raise requests.HTTPError(error_msg)
                
                # è§£æJSONå“åº”
                try:
                    result = response.json()
                except json.JSONDecodeError as e:
                    error_msg = f"JSONè§£æé”™è¯¯: {e}, å“åº”å†…å®¹: {response.text[:500]}"
                    print(f"JSONè§£æé”™è¯¯: {error_msg}")
                    raise ValueError(error_msg)
                
                # æå–å†…å®¹
                content = self._extract_content_from_response(result)
                
                print(f"APIè°ƒç”¨æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": content
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": result.get("usageMetadata", {})
                }
                
            except requests.Timeout as e:
                last_exception = e
                print(f"è¯·æ±‚è¶…æ—¶({self.timeout}ç§’), æ­£åœ¨é‡è¯•({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    break
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))
                
            except requests.HTTPError as e:
                last_exception = e
                print(f"HTTPé”™è¯¯: {e}, æ­£åœ¨é‡è¯•({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    break
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))
                
            except Exception as e:
                last_exception = e
                print(f"APIè°ƒç”¨å¤±è´¥: {e}, æ­£åœ¨é‡è¯•({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    break
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        error_msg = f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{LLM_CONFIG['retry_count']}æ¬¡"
        if last_exception:
            error_msg += f"ï¼Œæœ€åä¸€æ¬¡é”™è¯¯: {last_exception}"
        
        print(f"æœ€ç»ˆé”™è¯¯: {error_msg}")
        raise Exception(error_msg)

class PaperSummarizer:
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.client = ModelClient(api_key, model)
        self.max_papers_per_batch = 20 # é€‚å½“å‡å°‘æ‰¹å¤„ç†æ•°é‡ï¼Œé˜²æ­¢Promptè¿‡é•¿

    def _fix_markdown_links(self, text: str) -> str:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¿®å¤æœªæ­£ç¡®æ ¼å¼åŒ–çš„Markdowné“¾æ¥"""
        # æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ '### Title (http...)' æˆ– '### Title(http...)' æ ¼å¼
        # å®ƒä¼šæ•è·æ ‡é¢˜æ–‡æœ¬å’Œæ‹¬å·å†…çš„URL
        pattern = re.compile(r'^(###\s*)(.*?)\s*\((https?://[^\s)]+)\)$', re.MULTILINE)
        
        # æ›¿æ¢å‡½æ•°ï¼Œå°†æ•è·çš„ç»„é‡æ–°æ ¼å¼åŒ–ä¸º '### [Title](URL)'
        def replacer(match):
            prefix = match.group(1)  # '### '
            title = match.group(2).strip()  # æ ‡é¢˜
            url = match.group(3)  # URL
            return f'{prefix}[{title}]({url})'
            
        return pattern.sub(replacer, text)

    def _generate_batch_summaries(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """ä¸ºä¸€æ‰¹è®ºæ–‡ç”Ÿæˆæ€»ç»“"""
        batch_prompt = ""
        for i, paper in enumerate(papers, start=start_index):
            # ç¡®ä¿æ‘˜è¦åªå–ä¸€éƒ¨åˆ†ï¼Œé¿å…promptè¿‡é•¿
            summary_snippet = (paper['summary'][:800] + '...') if len(paper['summary']) > 800 else paper['summary']
            batch_prompt += f"""
---
è®ºæ–‡ {i}:
- æ ‡é¢˜: {paper['title']}
- ä½œè€…: {', '.join(paper['authors'])}
- å‘å¸ƒæ—¥æœŸ: {paper['published'][:10]}
- arXivé“¾æ¥: {paper['entry_id']}
- æ‘˜è¦: {summary_snippet}
"""
        
        final_prompt = f"""è¯·ä¸ºä»¥ä¸‹{len(papers)}ç¯‡æ¥è‡ªArXivçš„è®ºæ–‡ç”Ÿæˆä¸­æ–‡æ€»ç»“ã€‚æ¯ç¯‡è®ºæ–‡çš„æ€»ç»“éƒ½éœ€è¦éµå¾ªä¸¥æ ¼çš„Markdownæ ¼å¼ã€‚

**å¿…é¡»éµå¾ªçš„è¾“å‡ºæ ¼å¼:**
å¯¹äºæ¯ä¸€ç¯‡è®ºæ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ï¼Œä¸å¾—æœ‰ä»»ä½•å˜åŠ¨ï¼š

### [è®ºæ–‡æ ‡é¢˜](è®ºæ–‡çš„arXivé“¾æ¥)
<!-- è®ºæ–‡å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD -->
**ğŸ“… å‘å¸ƒæ—¥æœŸ**: è®ºæ–‡å‘å¸ƒæ—¥æœŸ(YYYY-MM-DDæ ¼å¼)

* **ğŸ‘¥ ä½œè€…**: ä½œè€…å
* **ğŸ¯ ç ”ç©¶ç›®çš„**: è¯¦ç»†æè¿°ç ”ç©¶çš„èƒŒæ™¯ã€åŠ¨æœºå’Œæ ¸å¿ƒç›®æ ‡ï¼ŒåŒ…æ‹¬è§£å†³çš„é—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
* **â­ ä¸»è¦å‘ç°**: è¯¦ç»†é˜è¿°è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€åˆ›æ–°ç‚¹ã€å®éªŒç»“æœå’Œç†è®ºçªç ´ï¼Œä»¥åŠå¯¹é¢†åŸŸçš„æ½œåœ¨å½±å“ã€‚

---

**å…³é”®æŒ‡ä»¤:**
1.  **æ— éœ€åºå·**: è®ºæ–‡æ ‡é¢˜å‰ä¸éœ€è¦æ·»åŠ åºå·ï¼Œåºå·å°†ç”±ç½‘ç«™é¡µé¢åŠ¨æ€ç”Ÿæˆã€‚
2.  **é“¾æ¥æ ¼å¼**: è®ºæ–‡æ ‡é¢˜å¿…é¡»ä½œä¸ºå¯ç‚¹å‡»çš„Markdowné“¾æ¥ï¼Œæ ¼å¼ä¸º `[æ ‡é¢˜](é“¾æ¥)`ã€‚
3.  **æ—¥æœŸæ³¨é‡Š**: åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œå¿…é¡»æ’å…¥HTMLæ³¨é‡Š `<!-- YYYY-MM-DD -->` æ¥æ ‡è®°å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ä¸¥æ ¼ä¸ºYYYY-MM-DDã€‚
4.  **å¯è§æ—¥æœŸ**: åœ¨HTMLæ³¨é‡Šåï¼Œå¿…é¡»æ·»åŠ å¯è§çš„æ—¥æœŸè¡Œï¼š`**ğŸ“… å‘å¸ƒæ—¥æœŸ**: YYYY-MM-DD`ã€‚
5.  **å†…å®¹ä¸°å¯Œ**: "ç ”ç©¶ç›®çš„"åº”åŒ…å«ç ”ç©¶èƒŒæ™¯ã€åŠ¨æœºå’Œç›®æ ‡ï¼›"ä¸»è¦å‘ç°"åº”è¯¦ç»†æè¿°æ ¸å¿ƒè´¡çŒ®ã€åˆ›æ–°ç‚¹å’Œå®éªŒç»“æœã€‚
6.  **åˆ†éš”ç¬¦**: æ¯ç¯‡è®ºæ–‡æ€»ç»“ä¹‹åï¼Œå¿…é¡»ä½¿ç”¨ `---` ä½œä¸ºåˆ†éš”ç¬¦ã€‚
7.  **è¯­è¨€**: æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä¸ºä¸­æ–‡ã€‚
8.  **æ•°å­¦å…¬å¼**: ä½ å¯ä»¥è‡ªç”±ä½¿ç”¨LaTeXè¯­æ³•ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰æ¥è¡¨ç¤ºæ•°å­¦å…¬å¼ã€‚
9.  **å®Œæ•´æ€§**: å¿…é¡»ä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆå®Œæ•´çš„æ‘˜è¦ï¼Œä¸å¾—é—æ¼ä»»ä½•ä¸€ç¯‡ã€‚

**éœ€è¦ä½ å¤„ç†çš„è®ºæ–‡ä¿¡æ¯å¦‚ä¸‹:**
{batch_prompt}
"""
        try:
            print(f"æ­£åœ¨ä¸º{len(papers)}ç¯‡è®ºæ–‡ç”Ÿæˆæ‘˜è¦...")
            response = self.client.chat_completion([{"role": "user", "content": final_prompt}])
            content = response["choices"][0]["message"]["content"].strip()
            
            # æ£€æŸ¥ç”Ÿæˆçš„å†…å®¹æ˜¯å¦å®Œæ•´
            generated_sections = content.count('###')
            expected_sections = len(papers)
            
            if generated_sections < expected_sections:
                print(f"è­¦å‘Š: ç”Ÿæˆçš„æ‘˜è¦æ•°é‡({generated_sections})å°‘äºé¢„æœŸ({expected_sections})")
                print(f"å¯èƒ½éƒ¨åˆ†è®ºæ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥")
            
            # åœ¨è¿”å›å†…å®¹åï¼Œç«‹å³è¿›è¡Œé“¾æ¥ä¿®å¤
            fixed_content = self._fix_markdown_links(content)
            print(f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œå…±ç”Ÿæˆ {generated_sections} ä¸ªæ‘˜è¦")
            return fixed_content
            
        except Exception as e:
            print(f"æ‰¹é‡ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            print(f"å°†å°è¯•ä¸ºæ¯ç¯‡è®ºæ–‡å•ç‹¬ç”Ÿæˆæ‘˜è¦...")
            return self._generate_individual_summaries(papers)

    def _generate_individual_summaries(self, papers: List[Dict[str, Any]]) -> str:
        """é€ä¸ªä¸ºè®ºæ–‡ç”Ÿæˆæ‘˜è¦ï¼ˆä½œä¸ºæ‰¹é‡å¤±è´¥çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        individual_summaries = []
        
        for i, paper in enumerate(papers):
            try:
                print(f"æ­£åœ¨ä¸ºç¬¬{i+1}ç¯‡è®ºæ–‡ç”Ÿæˆæ‘˜è¦: {paper['title'][:50]}...")
                
                # ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆæ‘˜è¦
                summary_snippet = (paper['summary'][:800] + '...') if len(paper['summary']) > 800 else paper['summary']
                
                single_prompt = f"""è¯·ä¸ºè¿™ç¯‡æ¥è‡ªArXivçš„è®ºæ–‡ç”Ÿæˆä¸­æ–‡æ€»ç»“ã€‚

**è®ºæ–‡ä¿¡æ¯:**
- æ ‡é¢˜: {paper['title']}
- ä½œè€…: {', '.join(paper['authors'])}
- å‘å¸ƒæ—¥æœŸ: {paper['published'][:10]}
- arXivé“¾æ¥: {paper['entry_id']}
- æ‘˜è¦: {summary_snippet}

**è¾“å‡ºæ ¼å¼:**
### [è®ºæ–‡æ ‡é¢˜](è®ºæ–‡çš„arXivé“¾æ¥)
<!-- è®ºæ–‡å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD -->
**ğŸ“… å‘å¸ƒæ—¥æœŸ**: è®ºæ–‡å‘å¸ƒæ—¥æœŸ(YYYY-MM-DDæ ¼å¼)

* **ğŸ‘¥ ä½œè€…**: ä½œè€…å
* **ğŸ¯ ç ”ç©¶ç›®çš„**: è¯¦ç»†æè¿°ç ”ç©¶çš„èƒŒæ™¯ã€åŠ¨æœºå’Œæ ¸å¿ƒç›®æ ‡ï¼ŒåŒ…æ‹¬è§£å†³çš„é—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
* **â­ ä¸»è¦å‘ç°**: è¯¦ç»†é˜è¿°è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€åˆ›æ–°ç‚¹ã€å®éªŒç»“æœå’Œç†è®ºçªç ´ï¼Œä»¥åŠå¯¹é¢†åŸŸçš„æ½œåœ¨å½±å“ã€‚

---

è¯·ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚ã€‚"""

                response = self.client.chat_completion([{"role": "user", "content": single_prompt}])
                content = response["choices"][0]["message"]["content"].strip()
                fixed_content = self._fix_markdown_links(content)
                individual_summaries.append(fixed_content)
                print(f"ç¬¬{i+1}ç¯‡è®ºæ–‡æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                
                # ä¸ºäº†é¿å…APIé™åˆ¶ï¼Œæ·»åŠ å°å»¶è¿Ÿ
                if i < len(papers) - 1:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"ç¬¬{i+1}ç¯‡è®ºæ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                # ç”Ÿæˆé”™è¯¯æ‘˜è¦
                error_summary = f"""### [{paper['title']}]({paper['entry_id']})
<!-- {paper['published'][:10]} -->
**ğŸ“… å‘å¸ƒæ—¥æœŸ**: {paper['published'][:10]}

* **ğŸ‘¥ ä½œè€…**: {', '.join(paper['authors'])}
* **ğŸ¯ ç ”ç©¶ç›®çš„**: ç”±äºAPIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶ç›®çš„æ‘˜è¦ã€‚è¯·å‚è€ƒåŸå§‹è®ºæ–‡äº†è§£è¯¦æƒ…ã€‚
* **â­ ä¸»è¦å‘ç°**: ç”±äºAPIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†çš„ä¸»è¦å‘ç°æ‘˜è¦ã€‚è¯·å‚è€ƒåŸå§‹è®ºæ–‡äº†è§£è¯¦æƒ…ã€‚

**é”™è¯¯ä¿¡æ¯**: {str(e)}

---"""
                individual_summaries.append(error_summary)
        
        return "\n".join(individual_summaries)

    def _process_batch(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """å¤„ç†ä¸€æ‰¹è®ºæ–‡"""
        print(f"æ­£åœ¨æ‰¹é‡å¤„ç† {len(papers)} ç¯‡è®ºæ–‡...")
        summaries = self._generate_batch_summaries(papers, start_index)
        
        # éªŒè¯ç”Ÿæˆçš„æ‘˜è¦æ•°é‡
        generated_count = summaries.count('###')
        expected_count = len(papers)
        
        if generated_count != expected_count:
            print(f"è­¦å‘Š: ç”Ÿæˆçš„æ‘˜è¦æ•°é‡({generated_count})ä¸é¢„æœŸ({expected_count})ä¸ç¬¦")
        
        time.sleep(2)  # ä¸ºäº†é¿å…APIé™åˆ¶
        return summaries

    def _validate_summaries(self, summaries: str, expected_count: int) -> bool:
        """éªŒè¯ç”Ÿæˆçš„æ‘˜è¦æ˜¯å¦å®Œæ•´"""
        actual_count = summaries.count('###')
        is_valid = actual_count == expected_count
        
        if not is_valid:
            print(f"æ‘˜è¦éªŒè¯å¤±è´¥: æœŸæœ›{expected_count}ç¯‡ï¼Œå®é™…{actual_count}ç¯‡")
        
        return is_valid

    def _generate_batch_summary(self, papers: List[Dict[str, Any]]) -> str:
        """æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è®ºæ–‡çš„æ€»ç»“"""
        all_summaries = []
        total_papers = len(papers)
        
        for i in range(0, total_papers, self.max_papers_per_batch):
            batch = papers[i:i + self.max_papers_per_batch]
            batch_size = len(batch)
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_papers)} ç¯‡è®ºæ–‡...")
            
            try:
                batch_summary = self._process_batch(batch, i + 1)
                
                # éªŒè¯æ‰¹æ¬¡æ‘˜è¦
                if self._validate_summaries(batch_summary, batch_size):
                    all_summaries.append(batch_summary)
                    print(f"æ‰¹æ¬¡å¤„ç†æˆåŠŸ: {batch_size}ç¯‡è®ºæ–‡")
                else:
                    print(f"æ‰¹æ¬¡éªŒè¯å¤±è´¥ï¼Œå°†é€ä¸ªå¤„ç†...")
                    individual_summary = self._generate_individual_summaries(batch)
                    all_summaries.append(individual_summary)
                    
            except Exception as e:
                print(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                print(f"å°†é€ä¸ªå¤„ç†è¿™{batch_size}ç¯‡è®ºæ–‡...")
                individual_summary = self._generate_individual_summaries(batch)
                all_summaries.append(individual_summary)
            
            # æ‰¹æ¬¡é—´ç­‰å¾…
            if i + self.max_papers_per_batch < total_papers:
                print(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾… {LLM_CONFIG['retry_delay']} ç§’åç»§ç»­...")
                time.sleep(LLM_CONFIG['retry_delay'])
        
        final_summary = "\n".join(all_summaries)
        
        # æœ€ç»ˆéªŒè¯
        if self._validate_summaries(final_summary, total_papers):
            print(f"âœ… æ‰€æœ‰{total_papers}ç¯‡è®ºæ–‡æ‘˜è¦ç”Ÿæˆå®Œæˆ")
        else:
            print(f"âš ï¸ éƒ¨åˆ†è®ºæ–‡æ‘˜è¦å¯èƒ½ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç»“æœ")
        
        return final_summary

    def summarize_papers(self, papers: List[Dict[str, Any]], output_file: str) -> bool:
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è®ºæ–‡å¹¶åˆ›å»ºMarkdownæŠ¥å‘Š"""
        print(f"å¼€å§‹ç”Ÿæˆè®ºæ–‡æ€»ç»“ï¼Œå…± {len(papers)} ç¯‡...")
        summaries = self._generate_batch_summary(papers)
        
        api_success = "[ç”Ÿæˆå¤±è´¥:" not in summaries
        if not api_success:
            print("è­¦å‘Š: æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œç»“æœå¯èƒ½ä¸å®Œæ•´")

        markdown_content = self._generate_markdown(papers, summaries)
        
        output_md = Path(output_file).with_suffix('.md')
        output_md.write_text(markdown_content, encoding='utf-8')
        print(f"Markdownæ–‡ä»¶å·²ä¿å­˜ï¼š{output_md}")
        
        return api_success

    def _generate_markdown(self, papers: List[Dict[str, Any]], summaries: str) -> str:
        """ç”Ÿæˆmarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        beijing_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
        
        return f"""# Arxivè®ºæ–‡æ€»ç»“æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- ç”Ÿæˆæ—¶é—´: {beijing_time}
- ä½¿ç”¨æ¨¡å‹: {self.client.model}
- è®ºæ–‡æ•°é‡: {len(papers)} ç¯‡

---

## è®ºæ–‡æ€»ç»“

{summaries}

---

## ç”Ÿæˆè¯´æ˜
- æœ¬æŠ¥å‘Šç”±AIæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼Œæ‘˜è¦å†…å®¹ä»…ä¾›å‚è€ƒã€‚
- å¦‚æœ‰é”™è¯¯æˆ–é—æ¼ï¼Œè¯·ä»¥åŸå§‹è®ºæ–‡ä¸ºå‡†ã€‚
"""