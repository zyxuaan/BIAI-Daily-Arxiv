"""
è®ºæ–‡æ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆè®ºæ–‡æ‘˜è¦
"""
import os
import re
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
import time
from datetime import datetime
import pytz
from config.settings import LLM_CONFIG

class ModelClient:
    """è¯­è¨€æ¨¡å‹APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or LLM_CONFIG['model']
        self.api_url = f"{LLM_CONFIG['api_url']}/{self.model}:generateContent"
        self.timeout = LLM_CONFIG.get('timeout', 60) # å¢åŠ è¶…æ—¶æ—¶é—´
        
    def _create_headers(self) -> Dict[str, str]:
        """åˆ›å»ºè¯·æ±‚å¤´"""
        return {
            "Content-Type": "application/json"
        }
    
    def _create_request_body(
        self, 
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºè¯·æ±‚ä½“"""
        prompt = messages[-1]["content"]
        
        return {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature or LLM_CONFIG['temperature'],
                "maxOutputTokens": max_tokens or LLM_CONFIG['max_output_tokens'],
                "topP": LLM_CONFIG['top_p'],
                "topK": LLM_CONFIG['top_k']
            }
        }
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºèŠå¤©å®Œæˆ"""
        headers = self._create_headers()
        data = self._create_request_body(messages, temperature, max_tokens)
        
        for attempt in range(LLM_CONFIG['retry_count']):
            try:
                response = requests.post(
                    f"{self.api_url}?key={self.api_key}",
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                    
                result = response.json()
                
                # æ£€æŸ¥è¿”å›å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                if not result.get("candidates") or not result["candidates"][0].get("content"):
                    raise ValueError("APIè¿”å›äº†æ— æ•ˆçš„å“åº”å†…å®¹")

                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": result["candidates"][0]["content"]["parts"][0]["text"]
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": result.get("usageMetadata", {})
                }
            except requests.Timeout:
                print(f"è¯·æ±‚è¶…æ—¶({self.timeout}ç§’), æ­£åœ¨é‡è¯•({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))
            except Exception as e:
                print(f"APIè°ƒç”¨å¤±è´¥: {e}, æ­£åœ¨é‡è¯•({attempt + 1}/{LLM_CONFIG['retry_count']})...")
                if attempt == LLM_CONFIG['retry_count'] - 1:
                    raise
                time.sleep(LLM_CONFIG['retry_delay'] * (2 ** attempt))

class PaperSummarizer:
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.client = ModelClient(api_key, model)
        self.max_papers_per_batch = 20 # é€‚å½“å‡å°‘æ‰¹å¤„ç†æ•°é‡ï¼Œé˜²æ­¢Promptè¿‡é•¿

    def _fix_markdown_links(self, text: str) -> str:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¿®å¤æœªæ­£ç¡®æ ¼å¼åŒ–çš„Markdowné“¾æ¥"""
        # æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ '### Title (http...)' æˆ– '### Title(http...)' æ ¼å¼
        # å®ƒä¼šæ•è·æ ‡é¢˜æ–‡æœ¬å’Œæ‹¬å·å†…çš„URL
        pattern = re.compile(r'^(###\s*)(.*?)\s*\((https?://[^\s)]+)\)$', re.MULTILINE)
        
        # æ›¿æ¢å‡½æ•°ï¼Œå°†æ•è·çš„ç»„é‡æ–°æ ¼å¼åŒ–ä¸º '[Title](URL)'
        def replacer(match):
            prefix = match.group(1)
            title = match.group(2).strip()
            url = match.group(3)
            return f'{prefix}[{title}]({url})'
            
        return pattern.sub(replacer, text)

    def _generate_batch_summaries(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """ä¸ºä¸€æ‰¹è®ºæ–‡ç”Ÿæˆæ€»ç»“"""
        batch_prompt = ""
        for i, paper in enumerate(papers, start=start_index):
            # ç¡®ä¿æ‘˜è¦åªå–ä¸€éƒ¨åˆ†ï¼Œé¿å…promptè¿‡é•¿
            summary_snippet = (paper['summary'][:800] + '...') if len(paper['summary']) > 800 else paper['summary']
            batch_prompt += f"""
---
è®ºæ–‡ {i}:
- æ ‡é¢˜: {paper['title']}
- ä½œè€…: {', '.join(paper['authors'])}
- å‘å¸ƒæ—¥æœŸ: {paper['published'][:10]}
- arXivé“¾æ¥: {paper['entry_id']}
- æ‘˜è¦: {summary_snippet}
"""
        
        final_prompt = f"""è¯·ä¸ºä»¥ä¸‹{len(papers)}ç¯‡æ¥è‡ªArXivçš„è®ºæ–‡ç”Ÿæˆä¸­æ–‡æ€»ç»“ã€‚æ¯ç¯‡è®ºæ–‡çš„æ€»ç»“éƒ½éœ€è¦éµå¾ªä¸¥æ ¼çš„Markdownæ ¼å¼ã€‚

**å¿…é¡»éµå¾ªçš„è¾“å‡ºæ ¼å¼:**
å¯¹äºæ¯ä¸€ç¯‡è®ºæ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ï¼Œä¸å¾—æœ‰ä»»ä½•å˜åŠ¨ï¼š

### [è®ºæ–‡æ ‡é¢˜](è®ºæ–‡çš„arXivé“¾æ¥)
<!-- è®ºæ–‡å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD -->
**ğŸ“… å‘å¸ƒæ—¥æœŸ**: è®ºæ–‡å‘å¸ƒæ—¥æœŸ(YYYY-MM-DDæ ¼å¼)

* **ğŸ‘¥ ä½œè€…**: ä½œè€…å
* **ğŸ¯ ç ”ç©¶ç›®çš„**: ä¸€å¥è¯æ€»ç»“ç ”ç©¶çš„æ ¸å¿ƒç›®æ ‡ã€‚
* **â­ ä¸»è¦å‘ç°**: ä¸€å¥è¯æ€»ç»“æœ€é‡è¦çš„å‘ç°æˆ–è´¡çŒ®ã€‚

---

**å…³é”®æŒ‡ä»¤:**
1.  **é“¾æ¥æ ¼å¼**: è®ºæ–‡æ ‡é¢˜å¿…é¡»ä½œä¸ºå¯ç‚¹å‡»çš„Markdowné“¾æ¥ï¼Œæ ¼å¼ä¸º `[æ ‡é¢˜](é“¾æ¥)`ã€‚
2.  **æ—¥æœŸæ³¨é‡Š**: åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œå¿…é¡»æ’å…¥HTMLæ³¨é‡Š `<!-- YYYY-MM-DD -->` æ¥æ ‡è®°å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ä¸¥æ ¼ä¸ºYYYY-MM-DDã€‚
3.  **å¯è§æ—¥æœŸ**: åœ¨HTMLæ³¨é‡Šåï¼Œå¿…é¡»æ·»åŠ å¯è§çš„æ—¥æœŸè¡Œï¼š`**ğŸ“… å‘å¸ƒæ—¥æœŸ**: YYYY-MM-DD`ã€‚
4.  **å†…å®¹**: "ç ”ç©¶ç›®çš„"å’Œ"ä¸»è¦å‘ç°"å¿…é¡»æ˜¯ç®€æ´çš„ä¸€å¥è¯æ€»ç»“ã€‚
5.  **åˆ†éš”ç¬¦**: æ¯ç¯‡è®ºæ–‡æ€»ç»“ä¹‹åï¼Œå¿…é¡»ä½¿ç”¨ `---` ä½œä¸ºåˆ†éš”ç¬¦ã€‚
6.  **è¯­è¨€**: æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä¸ºä¸­æ–‡ã€‚
7.  **æ•°å­¦å…¬å¼**: ä½ å¯ä»¥è‡ªç”±ä½¿ç”¨LaTeXè¯­æ³•ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰æ¥è¡¨ç¤ºæ•°å­¦å…¬å¼ã€‚

**éœ€è¦ä½ å¤„ç†çš„è®ºæ–‡ä¿¡æ¯å¦‚ä¸‹:**
{batch_prompt}
"""
        try:
            response = self.client.chat_completion([{"role": "user", "content": final_prompt}])
            content = response["choices"][0]["message"]["content"].strip()
            # åœ¨è¿”å›å†…å®¹åï¼Œç«‹å³è¿›è¡Œé“¾æ¥ä¿®å¤
            return self._fix_markdown_links(content)
        except Exception as e:
            error_msg = f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}]"
            return "\n".join([f"### {p['title']}\n<!-- {p['published'][:10]} -->\n**ğŸ“… å‘å¸ƒæ—¥æœŸ**: {p['published'][:10]}\n\n* **ğŸ‘¥ ä½œè€…**: {', '.join(p['authors'])}\n* **ğŸ¯ ç ”ç©¶ç›®çš„**: {error_msg}\n* **â­ ä¸»è¦å‘ç°**: {error_msg}\n\n---" for p in papers])

    def _process_batch(self, papers: List[Dict[str, Any]], start_index: int) -> str:
        """å¤„ç†ä¸€æ‰¹è®ºæ–‡"""
        print(f"æ­£åœ¨æ‰¹é‡å¤„ç† {len(papers)} ç¯‡è®ºæ–‡...")
        summaries = self._generate_batch_summaries(papers, start_index)
        time.sleep(2)
        return summaries

    def _generate_batch_summary(self, papers: List[Dict[str, Any]]) -> str:
        """æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è®ºæ–‡çš„æ€»ç»“"""
        all_summaries = []
        total_papers = len(papers)
        
        for i in range(0, total_papers, self.max_papers_per_batch):
            batch = papers[i:i + self.max_papers_per_batch]
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i + 1} åˆ° {min(i + self.max_papers_per_batch, total_papers)} ç¯‡è®ºæ–‡...")
            batch_summary = self._process_batch(batch, i + 1)
            all_summaries.append(batch_summary)
            
            if i + self.max_papers_per_batch < total_papers:
                print(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾… {LLM_CONFIG['retry_delay']} ç§’åç»§ç»­...")
                time.sleep(LLM_CONFIG['retry_delay'])
        
        return "\n".join(all_summaries)

    def summarize_papers(self, papers: List[Dict[str, Any]], output_file: str) -> bool:
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è®ºæ–‡å¹¶åˆ›å»ºMarkdownæŠ¥å‘Š"""
        print(f"å¼€å§‹ç”Ÿæˆè®ºæ–‡æ€»ç»“ï¼Œå…± {len(papers)} ç¯‡...")
        summaries = self._generate_batch_summary(papers)
        
        api_success = "[ç”Ÿæˆå¤±è´¥:" not in summaries
        if not api_success:
            print("è­¦å‘Š: æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œç»“æœå¯èƒ½ä¸å®Œæ•´")

        markdown_content = self._generate_markdown(papers, summaries)
        
        output_md = Path(output_file).with_suffix('.md')
        output_md.write_text(markdown_content, encoding='utf-8')
        print(f"Markdownæ–‡ä»¶å·²ä¿å­˜ï¼š{output_md}")
        
        return api_success

    def _generate_markdown(self, papers: List[Dict[str, Any]], summaries: str) -> str:
        """ç”Ÿæˆmarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        beijing_time = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
        
        return f"""# Arxivè®ºæ–‡æ€»ç»“æŠ¥å‘Š(Brain-inspired AI)

## åŸºæœ¬ä¿¡æ¯
- ç”Ÿæˆæ—¶é—´: {beijing_time}
- ä½¿ç”¨æ¨¡å‹: {self.client.model}
- è®ºæ–‡æ•°é‡: {len(papers)} ç¯‡

---

## è®ºæ–‡æ€»ç»“

{summaries}

---

## ç”Ÿæˆè¯´æ˜
- æœ¬æŠ¥å‘Šç”±AIæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼Œæ‘˜è¦å†…å®¹ä»…ä¾›å‚è€ƒã€‚
- å¦‚æœ‰é”™è¯¯æˆ–é—æ¼ï¼Œè¯·ä»¥åŸå§‹è®ºæ–‡ä¸ºå‡†ã€‚
"""