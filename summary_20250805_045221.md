---
layout: default
title: 2025-08-05 Arxiv论文摘要
---

# Arxiv论文总结报告

## 基本信息
- 生成时间: 2025-08-05 12:52:40
- 使用模型: gemini-2.5-flash
- 论文数量: 4 篇

---

## 论文总结

### [[Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection]](http://arxiv.org/abs/2508.02288v1)
<!-- 2025-08-04 -->
**📅 发布日期**: 2025-08-04

*   **👥 作者**: Jae-Young Kang, Hoonhee Cho, Kuk-Jin Yoon
*   **🎯 研究目的**: 3D目标检测对于自动驾驶系统至关重要，能够实现精确的定位和尺寸估计。然而，传统的LiDAR和RGB相机受限于固定的帧率，在高速场景下会产生感知盲区。事件相机凭借其异步特性和高时间分辨率，为连续捕获运动提供了解决方案。现有将事件相机与传统传感器结合的连续时间检测方法，由于依赖于同步传感器，在快速运动场景中表现不佳。本研究旨在提出一种仅依赖于立体事件相机的新型3D目标检测框架，以消除对传统3D传感器的需求，并解决事件数据中语义和几何信息不足的问题。
*   **⭐ 主要发现**: 本文提出了一种新颖的立体3D目标检测框架，该框架完全基于事件相机，从而消除了对传统3D传感器的依赖，解决了因传感器同步问题导致的在快速运动场景中的性能瓶颈。为了弥补事件数据中固有的语义和几何信息不足，研究引入了一种“双滤波器”机制（dual filter mechanism）。这一创新方法能够充分利用事件相机的高时间分辨率和异步特性，实现更鲁棒、更连续的3D目标检测，尤其适用于高动态环境下的自动驾驶系统。

---

### [[An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception]](http://arxiv.org/abs/2508.02238v1)
<!-- 2025-08-04 -->
**📅 发布日期**: 2025-08-04

*   **👥 作者**: Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu
*   **🎯 研究目的**: 事件相机因其宽动态范围、高时间分辨率和对运动模糊的免疫性等显著优势，在应对挑战性视觉条件方面展现出巨大潜力。然而，从异步事件流中提取和利用有效信息，对于事件相机在机载系统（如无人机）上的实时部署至关重要。本研究旨在提出一种简化的、基于事件的强度重建方案，以解决机载实现中的挑战，从而使传统的基于帧的视觉方法能够移植到事件相机场景，同时保留事件相机固有的优势。
*   **⭐ 主要发现**: 本文提出了一种名为“事件单次积分（event-based single integration, ESI）”的简化事件强度重建方案，专门用于解决无人机实时感知中的实现挑战。ESI方法通过对事件流进行单次积分，并结合一种增强机制（enhanced），来高效地重建强度图像。这一方案的关键优势在于，它保证了传统基于帧的视觉方法能够无缝移植到事件相机应用中，同时充分保留了事件相机固有的高时间分辨率和宽动态范围等优势，为无人机等机载平台的实时视觉感知提供了高效且鲁棒的解决方案。

---

### [[Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps]](http://arxiv.org/abs/2508.02127v1)
<!-- 2025-08-04 -->
**📅 发布日期**: 2025-08-04

*   **👥 作者**: Mingjie Liu, Hanqing Liu, Chuang Zhu
*   **🎯 研究目的**: 在恶劣光照条件下实现精确的目标检测对于自动驾驶等实际应用至关重要。尽管神经形态事件相机已被引入以应对这些场景，但恶劣光照常常会引发隧道壁或路面等产生的干扰性反射，这经常导致错误的障碍物检测。然而，无论是单独的RGB数据还是事件数据，都不足以鲁棒地解决这些复杂性，并且在不增加额外传感器的情况下缓解这些问题仍未得到充分探索。本研究旨在通过利用直接从单目RGB图像预测的法线图作为鲁棒的几何线索，来抑制虚假阳性并提高检测精度，从而克服这些挑战。
*   **⭐ 主要发现**: 本文提出了一种创新方法，通过引入从单目RGB图像直接预测的法线图作为鲁棒的几何线索，显著增强了恶劣光照条件下的目标检测性能。研究发现，这些法线图能够有效抑制由反射引起的虚假阳性检测，从而提高检测准确性。为此，作者提出了NRE-Net，一个新颖的多模态检测框架，该框架能够有效地融合RGB图像、事件数据以及新引入的法线图。这一方法在不增加额外物理传感器的情况下，解决了事件相机在恶劣光照下易受反射干扰的问题，为自动驾驶等应用提供了更可靠的感知能力。

---

### [[OmniEvent: Unified Event Representation Learning]](http://arxiv.org/abs/2508.01842v1)
<!-- 2025-08-03 -->
**📅 发布日期**: 2025-08-03

*   **👥 作者**: Weiqi Yan, Chenlu Lin, Youbiao Wang, Zhipeng Cai, Xiuhong Lin, Yangyang Shi, Weiquan Liu, Yu Zang
*   **🎯 研究目的**: 事件相机因其超高动态范围和时间分辨率在计算机视觉领域日益普及。然而，由于事件数据非结构化的分布和时空（S-T）不均匀性，现有的事件网络严重依赖于任务特定的设计，这使得现有架构难以在新任务中重用。本研究旨在提出OmniEvent，首个统一的事件表示学习框架，旨在实现跨多样化任务的SOTA性能，并彻底消除任务特定设计的需求。
*   **⭐ 主要发现**: 本文提出了OmniEvent，这是首个统一的事件表示学习框架，它在多种任务上实现了最先进（SOTA）的性能，并完全消除了对任务特定设计的需求。与以往将事件数据视为3D点云并手动调整时空缩放权重的方法不同，OmniEvent提出了一种新颖的“解耦-增强-融合（decouple-enhance-fuse）”范式。在该范式中，局部特征的聚合和增强在空间和时间域上独立进行，从而有效避免了时空不均匀性带来的问题。这一创新性工作为事件相机视觉应用提供了一个通用且高性能的架构，极大地简化了新任务的开发，并有望加速该领域的研究进展。

---

---

## 生成说明
- 本报告由AI模型自动生成，摘要内容仅供参考。
- 如有错误或遗漏，请以原始论文为准。
